{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13385986,"sourceType":"datasetVersion","datasetId":8493655}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Configuration and Data Loading\nimport polars as pl\nimport numpy as np\nfrom pathlib import Path\nfrom tqdm. auto import tqdm\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport catboost as cb\nimport warnings\nimport pickle\nimport zipfile\nimport json\nfrom datetime import datetime\n\nwarnings.filterwarnings('ignore')\n\n# Configuration\nCONFIG = {\n    'base_path': \"/kaggle/input/sih-isro/Data_SIH_2025/\",\n    'n_sites': 7,\n    'n_folds': 5,\n    'random_state': 42,\n    'targets': ['O3_target', 'NO2_target'],\n    'early_stopping': 100,\n    'iterations': 2000,\n    'depth': 12,\n    'learning_rate': 0.03,\n    'delhi_lat': 28.6,\n    'n_oof_iterations': 2,\n}\n\nprint(\"=\" * 80)\nprint(\"ENHANCED PIPELINE: CTM + SATELLITE CORRECTION + OOF LAGS\")\nprint(\"=\" * 80)\nprint(f\"Configuration:\")\nprint(f\"  Iterations: {CONFIG['iterations']}, Depth: {CONFIG['depth']}, LR: {CONFIG['learning_rate']}\")\nprint(f\"  OOF Iterations: {CONFIG['n_oof_iterations']}\")\nprint(f\"  CV Folds: {CONFIG['n_folds']}\")\n\n# Data Loading\nprint(\"\\n\" + \"-\" * 80)\nprint(\"STAGE 0: DATA LOADING AND PREPROCESSING\")\nprint(\"-\" * 80)\n\ndef load_data(config):\n    \"\"\"Load and combine all site data.\"\"\"\n    frames = []\n    for site_id in tqdm(range(1, config['n_sites'] + 1), desc=\"Loading sites\"):\n        path = Path(config['base_path']) / f\"site_{site_id}_train_data.csv\"\n        df = pl.read_csv(path)\n        df = df.with_columns(pl.lit(site_id). alias('site_id'))\n        frames.append(df)\n    \n    df = pl.concat(frames)\n    \n    # DateTime construction\n    df = df.with_columns([\n        pl.date(pl.col('year'). cast(pl.Int32), \n                pl.col('month').cast(pl.Int32), \n                pl.col('day').cast(pl.Int32)). alias('date')\n    ])\n    df = df.with_columns([\n        (pl.col('date'). cast(pl. Datetime) + \n         pl.duration(hours=pl.col('hour'). cast(pl.Int64))).alias('datetime')\n    ])\n    \n    return df. sort(['site_id', 'datetime'])\n\ndef fill_satellite_gaps(df):\n    \"\"\"Fill satellite gaps using daily propagation.\"\"\"\n    sat_cols = ['NO2_satellite', 'HCHO_satellite', 'ratio_satellite']\n    \n    # Daily aggregation\n    daily_sat = (\n        df.group_by(['site_id', 'date'])\n        .agg([pl.col(col).drop_nulls().first().alias(f'{col}_daily') for col in sat_cols])\n    )\n    \n    df = df.join(daily_sat, on=['site_id', 'date'], how='left')\n    \n    # Fill strategy\n    for col in sat_cols:\n        df = df.with_columns([\n            pl.coalesce([f'{col}_daily', col]). alias(f'{col}_filled')\n        ])\n        df = df.with_columns([\n            pl.col(f'{col}_filled'). forward_fill().over('site_id')\n        ])\n        df = df.with_columns([\n            pl.col(f'{col}_filled'). backward_fill().over('site_id')\n        ])\n    \n    # Satellite availability flag\n    df = df.with_columns([\n        pl.col('NO2_satellite'). is_not_null().cast(pl.Int8).alias('sat_available_flag')\n    ])\n    \n    # Drop daily columns\n    df = df.drop([f'{col}_daily' for col in sat_cols])\n    \n    return df\n\n# Load and preprocess\ndf = load_data(CONFIG)\ndf = fill_satellite_gaps(df)\n\nprint(f\"\\nData loaded: {df.shape[0]:,} samples x {df.shape[1]} columns\")\nprint(f\"Date range: {df['datetime'].min()} to {df['datetime']. max()}\")\nprint(f\"Sites: {df['site_id'].n_unique()}\")\n\n# Check target availability\nfor target in CONFIG['targets']:\n    n_valid = df[target].is_not_null(). sum()\n    pct = n_valid / len(df) * 100\n    print(f\"{target}: {n_valid:,} valid samples ({pct:.1f}%)\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:22:39.523692Z","iopub.execute_input":"2025-11-30T13:22:39.523869Z","iopub.status.idle":"2025-11-30T13:22:43.542155Z","shell.execute_reply.started":"2025-11-30T13:22:39.523853Z","shell.execute_reply":"2025-11-30T13:22:43.541384Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nENHANCED PIPELINE: CTM + SATELLITE CORRECTION + OOF LAGS\n================================================================================\nConfiguration:\n  Iterations: 2000, Depth: 12, LR: 0.03\n  OOF Iterations: 2\n  CV Folds: 5\n\n--------------------------------------------------------------------------------\nSTAGE 0: DATA LOADING AND PREPROCESSING\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading sites:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"767e4ccace274015bc9eacb343623d20"}},"metadata":{}},{"name":"stdout","text":"\nData loaded: 171,679 samples x 23 columns\nDate range: 2019-07-10 00:00:00 to 2024-06-30 00:00:00\nSites: 7\nO3_target: 171,679 valid samples (100.0%)\nNO2_target: 171,679 valid samples (100.0%)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 2: Comprehensive Feature Engineering (100+ Features)\nprint(\"\\n\" + \"=\" * 80)\nprint(\"STAGE 1: COMPREHENSIVE FEATURE ENGINEERING\")\nprint(\"=\" * 80)\n\ndef create_all_features(df, config):\n    \"\"\"Create all 100+ features for the model.\"\"\"\n    \n    lat_rad = config['delhi_lat'] * np.pi / 180\n    \n    # =========================================================================\n    # CATEGORY 1: TEMPORAL FEATURES (18 features)\n    # =========================================================================\n    print(\"\\n  [1/10] Creating temporal features...\")\n    \n    # Cyclical encodings\n    df = df.with_columns([\n        (pl.col('hour') * 2 * np.pi / 24).sin().alias('hour_sin'),\n        (pl.col('hour') * 2 * np.pi / 24).cos().alias('hour_cos'),\n        pl.col('date').dt.ordinal_day().alias('doy'),\n        pl.col('date').dt.weekday().alias('dayofweek'),\n    ])\n    \n    df = df.with_columns([\n        (pl.col('doy') * 2 * np.pi / 365).sin().alias('doy_sin'),\n        (pl.col('doy') * 2 * np.pi / 365).cos().alias('doy_cos'),\n        (pl.col('dayofweek') * 2 * np.pi / 7).sin().alias('dow_sin'),\n        (pl.col('dayofweek') * 2 * np.pi / 7).cos().alias('dow_cos'),\n    ])\n    \n    # Weekend and rush hour\n    df = df.with_columns([\n        (pl.col('dayofweek') >= 5).cast(pl.Int8).alias('is_weekend'),\n        ((pl.col('hour') >= 7) & (pl.col('hour') <= 10)).cast(pl.Int8).alias('is_morning_rush'),\n        ((pl.col('hour') >= 17) & (pl.col('hour') <= 20)).cast(pl.Int8).alias('is_evening_rush'),\n    ])\n    \n    df = df.with_columns([\n        (pl.col('is_morning_rush') | pl.col('is_evening_rush')).cast(pl.Int8).alias('is_rush_hour')\n    ])\n    \n    # Diurnal phase (0-5)\n    df = df.with_columns([\n        pl.when(pl.col('hour') < 6).then(0)\n        .when(pl.col('hour') < 10).then(1)\n        .when(pl.col('hour') < 14).then(2)\n        .when(pl.col('hour') < 18).then(3)\n        .when(pl.col('hour') < 22).then(4)\n        .otherwise(5)\n        .alias('diurnal_phase')\n    ])\n    \n    # Seasonal indicators\n    df = df.with_columns([\n        pl.col('month').is_in([12, 1, 2]).cast(pl.Int8).alias('is_winter'),\n        pl.col('month').is_in([3, 4, 5]).cast(pl.Int8).alias('is_summer'),\n        pl.col('month').is_in([6, 7, 8, 9]).cast(pl.Int8).alias('is_monsoon'),\n        pl.col('month').is_in([10, 11]).cast(pl.Int8).alias('is_post_monsoon'),\n    ])\n    \n    # =========================================================================\n    # CATEGORY 2: SOLAR GEOMETRY (4 features)\n    # =========================================================================\n    print(\"  [2/10] Creating solar geometry features...\")\n    \n    df = df.with_columns([\n        (23.45 * np.pi / 180 * (2 * np.pi * (pl.col('doy') - 81) / 365).sin()).alias('declination'),\n        ((pl.col('hour') - 12) * 15 * np.pi / 180).alias('hour_angle'),\n    ])\n    \n    df = df.with_columns([\n        (np.sin(lat_rad) * pl.col('declination').sin() + \n         np.cos(lat_rad) * pl.col('declination').cos() * pl.col('hour_angle').cos()\n        ).clip(0, 1).alias('cos_sza')\n    ])\n    \n    df = df.with_columns([\n        pl.col('cos_sza').pow(2).alias('cos_sza_squared'),\n        (pl.col('cos_sza') > 0.1).cast(pl.Int8).alias('is_daytime'),\n    ])\n    \n    # =========================================================================\n    # CATEGORY 3: METEOROLOGICAL DERIVED (20 features)\n    # =========================================================================\n    print(\"  [3/10] Creating meteorological features...\")\n    \n    # Basic derived\n    df = df.with_columns([\n        (pl.col('T_forecast') + 273.15).alias('T_kelvin'),\n        (pl.col('u_forecast').pow(2) + pl.col('v_forecast').pow(2)).sqrt().alias('wind_speed'),\n    ])\n    \n    # Wind direction\n    df = df.with_columns([\n        pl.arctan2(pl.col('v_forecast'), pl.col('u_forecast')).alias('wind_dir_rad'),\n    ])\n    \n    df = df.with_columns([\n        pl.col('wind_dir_rad').sin().alias('wind_dir_sin'),\n        pl.col('wind_dir_rad').cos().alias('wind_dir_cos'),\n    ])\n    \n    # Dispersion and stability\n    df = df.with_columns([\n        (pl.col('wind_speed') * pl.col('w_forecast').abs().clip(0.01, None)).alias('ventilation'),\n        (pl.col('T_forecast') * pl.col('w_forecast').abs().clip(0.01, None)).alias('pbl_proxy'),\n        (pl.col('T_forecast') / (pl.col('q_forecast').clip(0.1, None))).alias('thermal_stability'),\n        (pl.col('w_forecast') / (pl.col('wind_speed').clip(0.1, None))).alias('vertical_wind_ratio'),\n    ])\n    \n    # Temperature derived\n    df = df.with_columns([\n        pl.col('T_forecast').pow(2).alias('T_squared'),\n        pl.col('q_forecast').pow(2).alias('q_squared'),\n        (pl.col('T_forecast') * pl.col('q_forecast')).alias('T_q_interaction'),\n    ])\n    \n    # Wind categories\n    df = df.with_columns([\n        (pl.col('wind_speed') < 1.0).cast(pl.Int8).alias('wind_calm'),\n        ((pl.col('wind_speed') >= 1.0) & (pl.col('wind_speed') < 3.0)).cast(pl.Int8).alias('wind_light'),\n        ((pl.col('wind_speed') >= 3.0) & (pl.col('wind_speed') < 6.0)).cast(pl.Int8).alias('wind_moderate'),\n        (pl.col('wind_speed') >= 6.0).cast(pl.Int8).alias('wind_strong'),\n    ])\n    \n    # Stagnation\n    df = df.with_columns([\n        ((pl.col('wind_speed') < 2.0) & (pl.col('w_forecast').abs() < 0.5)).cast(pl.Int8).alias('is_stagnant')\n    ])\n    \n    # =========================================================================\n    # CATEGORY 4: PHOTOCHEMICAL FEATURES (8 features)\n    # =========================================================================\n    print(\"  [4/10] Creating photochemical features...\")\n    \n    df = df.with_columns([\n        (-1500 / pl.col('T_kelvin')).exp().alias('arrhenius_1500'),\n        (-2500 / pl.col('T_kelvin')).exp().alias('arrhenius_2500'),\n        (pl.col('cos_sza') * pl.col('T_forecast')).alias('photolysis_potential'),\n    ])\n    \n    df = df.with_columns([\n        (pl.col('cos_sza') * pl.col('T_forecast') * pl.col('arrhenius_1500')).alias('o3_prod_potential'),\n        (pl.col('NO2_forecast') * (1 - pl.col('cos_sza'))).alias('nox_titration_proxy'),\n        (pl.col('is_daytime') * pl.col('T_forecast')).alias('daytime_T'),\n        (pl.col('is_daytime') * pl.col('wind_speed')).alias('daytime_wind'),\n    ])\n    \n    # =========================================================================\n    # CATEGORY 5: SATELLITE DERIVED (12 features)\n    # =========================================================================\n    print(\"  [5/10] Creating satellite-derived features...\")\n    \n    # VOC-NOx ratio\n    df = df.with_columns([\n        (pl.col('HCHO_satellite_filled') / (pl.col('NO2_satellite_filled').clip(0.01, None))).alias('voc_nox_ratio')\n    ])\n    \n    # Chemistry regime indicators\n    df = df.with_columns([\n        (pl.col('voc_nox_ratio') < 1.0).cast(pl.Int8).alias('is_voc_limited'),\n        (pl.col('voc_nox_ratio') > 2.0).cast(pl.Int8).alias('is_nox_limited'),\n        ((pl.col('voc_nox_ratio') >= 1.0) & (pl.col('voc_nox_ratio') <= 2.0)).cast(pl.Int8).alias('is_transitional'),\n    ])\n    \n    # Satellite-radiation interactions\n    df = df.with_columns([\n        (pl.col('cos_sza') * pl.col('NO2_satellite_filled')).alias('radiation_no2'),\n        (pl.col('cos_sza') * pl.col('HCHO_satellite_filled')).alias('radiation_hcho'),\n    ])\n    \n    # Satellite-meteorology interactions\n    df = df.with_columns([\n        (pl.col('T_forecast') * pl.col('NO2_satellite_filled')).alias('T_no2_sat'),\n        (pl.col('T_forecast') * pl.col('HCHO_satellite_filled')).alias('T_hcho_sat'),\n        (pl.col('ventilation') * pl.col('NO2_satellite_filled')).alias('vent_no2_sat'),\n        (pl.col('ventilation') * pl.col('HCHO_satellite_filled')).alias('vent_hcho_sat'),\n    ])\n    \n    # =========================================================================\n    # CATEGORY 6: CTM FEATURES (12 features)\n    # =========================================================================\n    print(\"  [6/10] Creating CTM-derived features...\")\n    \n    # CTM interactions\n    df = df.with_columns([\n        (pl.col('O3_forecast') * pl.col('cos_sza')).alias('o3_fcst_radiation'),\n        (pl.col('NO2_forecast') * pl.col('cos_sza')).alias('no2_fcst_radiation'),\n        (pl.col('O3_forecast') * pl.col('T_forecast')).alias('o3_fcst_T'),\n        (pl.col('NO2_forecast') * pl.col('T_forecast')).alias('no2_fcst_T'),\n        (pl.col('O3_forecast') * pl.col('ventilation')).alias('o3_fcst_vent'),\n        (pl.col('NO2_forecast') * pl.col('ventilation')).alias('no2_fcst_vent'),\n        (pl.col('NO2_forecast') * pl.col('is_rush_hour')).alias('no2_fcst_rush'),\n    ])\n    \n    # CTM ratios and squared\n    df = df.with_columns([\n        (pl.col('O3_forecast') / (pl.col('NO2_forecast').clip(0.1, None))).alias('ctm_o3_no2_ratio'),\n        pl.col('O3_forecast').pow(2).alias('o3_fcst_squared'),\n        pl.col('NO2_forecast').pow(2).alias('no2_fcst_squared'),\n    ])\n    \n    # Add a final interaction feature (O3/Met)\n    df = df.with_columns([\n        (pl.col('O3_forecast') * pl.col('thermal_stability')).alias('o3_fcst_stability')\n    ])\n    \n    # =========================================================================\n    # CATEGORY 7: ROLLING METEOROLOGICAL (18 features)\n    # =========================================================================\n    print(\"  [7/10] Creating rolling meteorological features...\")\n    \n    df = df.sort(['site_id', 'datetime'])\n    \n    for col, windows in [('T_forecast', [6, 24]), ('wind_speed', [6, 24]), \n                          ('ventilation', [6, 24]), ('cos_sza', [6])]:\n        for w in windows:\n            # Check if rolling std is needed (cos_sza std is usually near zero)\n            is_std_needed = col in ['T_forecast', 'wind_speed', 'ventilation']\n            \n            df = df.with_columns([\n                pl.col(col).rolling_mean(window_size=w, min_periods=1).over('site_id').alias(f'{col}_rmean_{w}h'),\n            ])\n            if is_std_needed:\n                df = df.with_columns([\n                    pl.col(col).rolling_std(window_size=w, min_periods=2).over('site_id').alias(f'{col}_rstd_{w}h'),\n                ])\n            \n    # Trends\n    df = df.with_columns([\n        (pl.col('T_forecast') - pl.col('T_forecast_rmean_6h')).alias('T_trend_6h'),\n        (pl.col('wind_speed') - pl.col('wind_speed_rmean_6h')).alias('wind_trend_6h'),\n        (pl.col('ventilation') - pl.col('ventilation_rmean_6h')).alias('vent_trend_6h'),\n    ])\n    \n    # Fill rolling NaNs (e.g., initial observations)\n    rolling_cols = [c for c in df.columns if '_rmean_' in c or '_rstd_' in c or '_trend_' in c]\n    for col in rolling_cols:\n        df = df.with_columns(pl.col(col).fill_null(strategy='forward').over('site_id').fill_null(0).alias(col))\n        \n    # =========================================================================\n    # CATEGORY 8: GAP AWARENESS (5 features)\n    # =========================================================================\n    print(\"  [8/10] Creating gap awareness features...\")\n    \n    df = df.with_columns([\n        pl.col('datetime').diff().over('site_id').dt.total_hours().fill_null(1.0).alias('hours_since_prev_obs')\n    ])\n    \n    df = df.with_columns([\n        (pl.col('hours_since_prev_obs') > 1).cast(pl.Int8).alias('has_gap'),\n        (pl.col('hours_since_prev_obs') > 6).cast(pl.Int8).alias('has_gap_6h'),\n        (pl.col('hours_since_prev_obs') > 24).cast(pl.Int8).alias('has_gap_24h'),\n        (pl.col('hours_since_prev_obs') + 1).log().alias('log_hours_since_prev'),\n    ])\n    \n    # =========================================================================\n    # CATEGORY 9: SITE FEATURES (7 features)\n    # =========================================================================\n    print(\"  [9/10] Creating site indicator features...\")\n    \n    for site_id in range(1, config['n_sites'] + 1):\n        df = df.with_columns([\n            (pl.col('site_id') == site_id).cast(pl.Int8).alias(f'site_{site_id}')\n        ])\n    \n    # =========================================================================\n    # CLEANUP\n    # =========================================================================\n    print(\"  [10/10] Cleaning up intermediate columns...\")\n    \n    # Drop intermediate columns\n    drop_cols = ['declination', 'hour_angle', 'wind_dir_rad', 'T_kelvin', 'doy']\n    df = df.drop([c for c in drop_cols if c in df.columns])\n    \n    return df\n\n# Create features\n# NOTE: df and CONFIG must be defined in the calling environment.\ndf = create_all_features(df, CONFIG) \n\n# Count features by category (assuming df is now the result)\nfeature_cols = [c for c in df.columns if c not in [\n    'year', 'month', 'day', 'hour', 'date', 'datetime', \n    'O3_target', 'NO2_target', 'site_id',\n    'NO2_satellite', 'HCHO_satellite', 'ratio_satellite'\n]]\n\nprint(f\"\\n  Total features created: {len(feature_cols)}\")\nprint(f\"  DataFrame shape: {df.shape[0]:,} x {df.shape[1]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:27:22.983601Z","iopub.execute_input":"2025-11-30T13:27:22.984266Z","iopub.status.idle":"2025-11-30T13:27:23.200607Z","shell.execute_reply.started":"2025-11-30T13:27:22.984241Z","shell.execute_reply":"2025-11-30T13:27:23.199822Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nSTAGE 1: COMPREHENSIVE FEATURE ENGINEERING\n================================================================================\n\n  [1/10] Creating temporal features...\n  [2/10] Creating solar geometry features...\n  [3/10] Creating meteorological features...\n  [4/10] Creating photochemical features...\n  [5/10] Creating satellite-derived features...\n  [6/10] Creating CTM-derived features...\n  [7/10] Creating rolling meteorological features...\n  [8/10] Creating gap awareness features...\n  [9/10] Creating site indicator features...\n  [10/10] Cleaning up intermediate columns...\n\n  Total features created: 101\n  DataFrame shape: 171,679 x 113\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 3: CTM Bias Correction\nprint(\"\\n\" + \"=\" * 80)\nprint(\"STAGE 2: CTM BIAS CORRECTION\")\nprint(\"=\" * 80)\n\ndef compute_metrics(y_true, y_pred):\n    \"\"\"Compute regression metrics.\"\"\"\n    mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n    yt, yp = y_true[mask], y_pred[mask]\n    if len(yt) == 0:\n        return {'rmse': np. nan, 'mae': np.nan, 'r2': np.nan, 'corr': np.nan, 'bias': np.nan}\n    \n    rmse = np.sqrt(mean_squared_error(yt, yp))\n    mae = mean_absolute_error(yt, yp)\n    r2 = r2_score(yt, yp)\n    corr = np.corrcoef(yt, yp)[0, 1] if len(yt) > 1 else 0\n    bias = np.mean(yp - yt)\n    \n    return {'rmse': rmse, 'mae': mae, 'r2': r2, 'corr': corr, 'bias': bias}\n\ndef train_ctm_correction(df, target_name, forecast_name, config):\n    \"\"\"Train a model to correct CTM bias.\"\"\"\n    \n    print(f\"\\n  Correcting {forecast_name} for {target_name}...\")\n    \n    # Features for bias prediction (regime features)\n    bias_features = [\n        forecast_name, 'hour_sin', 'hour_cos', 'month',\n        'doy_sin', 'doy_cos', 'T_forecast', 'q_forecast',\n        'wind_speed', 'ventilation', 'cos_sza', 'cos_sza_squared',\n        'is_daytime', 'is_winter', 'is_monsoon', 'diurnal_phase',\n        'thermal_stability', 'is_stagnant'\n    ] + [f'site_{i}' for i in range(1, config['n_sites'] + 1)]\n    \n    bias_features = [f for f in bias_features if f in df. columns]\n    \n    # Filter valid samples\n    df_valid = df. filter(\n        pl.col(target_name).is_not_null() & \n        pl.col(forecast_name).is_not_null()\n    )\n    \n    # Get arrays\n    X = df_valid. select(bias_features).to_numpy(). astype(np.float32)\n    y_actual = df_valid[target_name].to_numpy().astype(np.float32)\n    y_forecast = df_valid[forecast_name].to_numpy(). astype(np. float32)\n    y_bias = y_forecast - y_actual  # Target is the bias\n    \n    # Handle NaN\n    col_medians = np.nanmedian(X, axis=0)\n    col_medians = np.where(np.isnan(col_medians), 0, col_medians)\n    for j in range(X.shape[1]):\n        X[np.isnan(X[:, j]), j] = col_medians[j]\n    \n    # Raw CTM metrics\n    raw_metrics = compute_metrics(y_actual, y_forecast)\n    print(f\"    Raw CTM -> Correlation: {raw_metrics['corr']:.4f}, RMSE: {raw_metrics['rmse']:.2f}, Bias: {raw_metrics['bias']:+.2f}\")\n    \n    # Train bias correction model with OOF\n    kf = KFold(n_splits=config['n_folds'], shuffle=True, random_state=config['random_state'])\n    oof_bias_pred = np.zeros(len(y_bias))\n    models = []\n    \n    for fold_idx, (train_idx, val_idx) in enumerate(tqdm(kf. split(X), total=config['n_folds'], \n                                                          desc=f\"    Training bias model\", leave=False)):\n        model = cb.CatBoostRegressor(\n            iterations=500,\n            depth=8,\n            learning_rate=0.05,\n            l2_leaf_reg=5,\n            loss_function='RMSE',\n            random_seed=config['random_state'] + fold_idx,\n            verbose=False,\n            early_stopping_rounds=50,\n            task_type='GPU',\n            devices = \"0:1\"\n        )\n        model.fit(X[train_idx], y_bias[train_idx], \n                  eval_set=(X[val_idx], y_bias[val_idx]), verbose=False)\n        oof_bias_pred[val_idx] = model.predict(X[val_idx])\n        models. append(model)\n    \n    # Corrected forecast\n    y_corrected = y_forecast - oof_bias_pred\n    \n    # Corrected metrics\n    corrected_metrics = compute_metrics(y_actual, y_corrected)\n    print(f\"    Corrected -> Correlation: {corrected_metrics['corr']:.4f}, RMSE: {corrected_metrics['rmse']:.2f}, Bias: {corrected_metrics['bias']:+.2f}\")\n    print(f\"    Improvement: Corr {corrected_metrics['corr'] - raw_metrics['corr']:+.4f}, RMSE {raw_metrics['rmse'] - corrected_metrics['rmse']:+.2f}\")\n    \n    return {\n        'models': models,\n        'features': bias_features,\n        'col_medians': col_medians,\n        'raw_metrics': raw_metrics,\n        'corrected_metrics': corrected_metrics,\n        'oof_bias_pred': oof_bias_pred,\n        'forecast_name': forecast_name,\n    }\n\n# Define target-forecast mapping\ntarget_forecast_map = {\n    'O3_target': 'O3_forecast',\n    'NO2_target': 'NO2_forecast'\n}\n\n# Train CTM correction for both targets\nctm_corrections = {}\n\nfor target in CONFIG['targets']:\n    forecast = target_forecast_map[target]\n    ctm_corrections[target] = train_ctm_correction(df, target, forecast, CONFIG)\n\n# Apply corrections to full dataframe\nprint(\"\\n  Applying CTM corrections to full dataset...\")\n\ndef apply_ctm_correction(df, correction_result):\n    \"\"\"Apply trained correction to dataframe.\"\"\"\n    features = correction_result['features']\n    models = correction_result['models']\n    col_medians = correction_result['col_medians']\n    forecast_name = correction_result['forecast_name']\n    \n    # Prepare features\n    available_features = [f for f in features if f in df.columns]\n    X = df. select(available_features).to_numpy(). astype(np. float32)\n    \n    # Handle NaN - need to match feature order\n    for j in range(X.shape[1]):\n        if j < len(col_medians):\n            X[np.isnan(X[:, j]), j] = col_medians[j]\n        else:\n            X[np.isnan(X[:, j]), j] = 0\n    \n    # Average predictions across fold models\n    bias_pred = np. zeros(len(X))\n    for model in models:\n        bias_pred += model.predict(X) / len(models)\n    \n    # Corrected forecast\n    forecast_vals = df[forecast_name]. to_numpy()\n    corrected = forecast_vals - bias_pred\n    \n    return corrected\n\n# Add corrected CTM columns\no3_corrected = apply_ctm_correction(df, ctm_corrections['O3_target'])\nno2_corrected = apply_ctm_correction(df, ctm_corrections['NO2_target'])\n\ndf = df.with_columns([\n    pl. Series('O3_forecast_corrected', o3_corrected),\n    pl.Series('NO2_forecast_corrected', no2_corrected),\n])\n\n# Create corrected CTM interaction features\ndf = df.with_columns([\n    (pl.col('O3_forecast_corrected') * pl. col('cos_sza')).alias('o3_corr_radiation'),\n    (pl.col('NO2_forecast_corrected') * pl. col('cos_sza')).alias('no2_corr_radiation'),\n    (pl.col('O3_forecast_corrected') * pl.col('ventilation')).alias('o3_corr_vent'),\n    (pl.col('NO2_forecast_corrected') * pl. col('ventilation')).alias('no2_corr_vent'),\n    (pl.col('O3_forecast_corrected') / (pl.col('NO2_forecast_corrected'). clip(0.1, None))).alias('ctm_corr_ratio'),\n])\n\nprint(f\"  CTM correction complete. New columns added.\")\nprint(f\"  DataFrame shape: {df. shape[0]:,} x {df.shape[1]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:27:31.515758Z","iopub.execute_input":"2025-11-30T13:27:31.516357Z","iopub.status.idle":"2025-11-30T13:28:21.159359Z","shell.execute_reply.started":"2025-11-30T13:27:31.516320Z","shell.execute_reply":"2025-11-30T13:28:21.158633Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nSTAGE 2: CTM BIAS CORRECTION\n================================================================================\n\n  Correcting O3_forecast for O3_target...\n    Raw CTM -> Correlation: 0.0603, RMSE: 78.97, Bias: +38.88\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    Training bias model:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"    Corrected -> Correlation: 0.8290, RMSE: 18.99, Bias: -0.01\n    Improvement: Corr +0.7687, RMSE +59.98\n\n  Correcting NO2_forecast for NO2_target...\n    Raw CTM -> Correlation: -0.0308, RMSE: 56.25, Bias: +19.18\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    Training bias model:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"    Corrected -> Correlation: 0.7946, RMSE: 17.84, Bias: -0.00\n    Improvement: Corr +0.8254, RMSE +38.41\n\n  Applying CTM corrections to full dataset...\n  CTM correction complete. New columns added.\n  DataFrame shape: 171,679 x 120\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 4: Satellite Diurnal Adjustment\nprint(\"\\n\" + \"=\" * 80)\nprint(\"STAGE 3: SATELLITE DIURNAL ADJUSTMENT\")\nprint(\"=\" * 80)\n\ndef train_satellite_adjustment(df, target_name, sat_col, config):\n    \"\"\"Train a model to adjust daily satellite values to hourly estimates.\"\"\"\n    \n    print(f\"\\n  Adjusting {sat_col} for {target_name}...\")\n    \n    # Features for hourly adjustment\n    adj_features = [\n        sat_col, 'hour_sin', 'hour_cos', 'hour',\n        'doy_sin', 'doy_cos', 'month',\n        'T_forecast', 'wind_speed', 'ventilation',\n        'cos_sza', 'cos_sza_squared', 'is_daytime',\n        'is_winter', 'is_monsoon', 'is_summer',\n        'diurnal_phase', 'is_rush_hour', 'is_stagnant',\n        'thermal_stability', 'pbl_proxy',\n    ] + [f'site_{i}' for i in range(1, config['n_sites'] + 1)]\n    \n    adj_features = [f for f in adj_features if f in df.columns]\n    \n    # Filter valid samples\n    df_valid = df.filter(\n        pl.col(target_name).is_not_null() & \n        pl. col(sat_col).is_not_null()\n    )\n    \n    # Get arrays\n    X = df_valid. select(adj_features).to_numpy(). astype(np. float32)\n    y_actual = df_valid[target_name].to_numpy().astype(np.float32)\n    y_sat = df_valid[sat_col].to_numpy().astype(np.float32)\n    \n    # Target: adjustment ratio (how to scale satellite to match actual)\n    # We predict the ratio: actual / satellite, then multiply satellite by this ratio\n    # But to avoid division issues, we predict actual directly conditioned on satellite\n    y_target = y_actual  # Predict actual given satellite and regime\n    \n    # Handle NaN\n    col_medians = np.nanmedian(X, axis=0)\n    col_medians = np.where(np. isnan(col_medians), 0, col_medians)\n    for j in range(X. shape[1]):\n        X[np. isnan(X[:, j]), j] = col_medians[j]\n    \n    # Raw satellite correlation with target\n    raw_corr = np.corrcoef(y_sat, y_actual)[0, 1]\n    raw_rmse = np.sqrt(mean_squared_error(y_actual, y_sat))\n    print(f\"    Raw satellite -> Correlation: {raw_corr:.4f}, RMSE: {raw_rmse:.2f}\")\n    \n    # Train adjustment model with OOF\n    kf = KFold(n_splits=config['n_folds'], shuffle=True, random_state=config['random_state'])\n    oof_adjusted = np.zeros(len(y_target))\n    models = []\n    \n    for fold_idx, (train_idx, val_idx) in enumerate(tqdm(kf.split(X), total=config['n_folds'],\n                                                          desc=f\"    Training adjustment model\", leave=False)):\n        model = cb.CatBoostRegressor(\n            iterations=500,\n            depth=8,\n            learning_rate=0.05,\n            l2_leaf_reg=5,\n            loss_function='RMSE',\n            random_seed=config['random_state'] + fold_idx + 100,\n            verbose=False,\n            early_stopping_rounds=50,\n            task_type='GPU',\n            devices = \"0:1\"\n        )\n        model.fit(X[train_idx], y_target[train_idx],\n                  eval_set=(X[val_idx], y_target[val_idx]), verbose=False)\n        oof_adjusted[val_idx] = model.predict(X[val_idx])\n        models.append(model)\n    \n    # Adjusted metrics\n    adj_corr = np.corrcoef(oof_adjusted, y_actual)[0, 1]\n    adj_rmse = np.sqrt(mean_squared_error(y_actual, oof_adjusted))\n    print(f\"    Adjusted satellite -> Correlation: {adj_corr:.4f}, RMSE: {adj_rmse:.2f}\")\n    print(f\"    Improvement: Corr {adj_corr - raw_corr:+.4f}, RMSE {raw_rmse - adj_rmse:+.2f}\")\n    \n    return {\n        'models': models,\n        'features': adj_features,\n        'col_medians': col_medians,\n        'raw_corr': raw_corr,\n        'adj_corr': adj_corr,\n        'raw_rmse': raw_rmse,\n        'adj_rmse': adj_rmse,\n    }\n\n# Train satellite adjustment for each target-satellite pair\nsat_adjustments = {}\n\n# For O3: Use both NO2 and HCHO satellite\nsat_adjustments['O3_NO2_sat'] = train_satellite_adjustment(\n    df, 'O3_target', 'NO2_satellite_filled', CONFIG)\nsat_adjustments['O3_HCHO_sat'] = train_satellite_adjustment(\n    df, 'O3_target', 'HCHO_satellite_filled', CONFIG)\n\n# For NO2: Use NO2 satellite\nsat_adjustments['NO2_NO2_sat'] = train_satellite_adjustment(\n    df, 'NO2_target', 'NO2_satellite_filled', CONFIG)\n\n# Apply adjustments\nprint(\"\\n  Applying satellite adjustments to full dataset...\")\n\ndef apply_satellite_adjustment(df, adj_result):\n    \"\"\"Apply trained adjustment to dataframe.\"\"\"\n    features = adj_result['features']\n    models = adj_result['models']\n    col_medians = adj_result['col_medians']\n    \n    X = df.select([f for f in features if f in df. columns]).to_numpy().astype(np.float32)\n    for j in range(X.shape[1]):\n        X[np.isnan(X[:, j]), j] = col_medians[j]\n    \n    adjusted = np.zeros(len(X))\n    for model in models:\n        adjusted += model.predict(X) / len(models)\n    \n    return adjusted\n\n# Add adjusted satellite columns\ndf = df.with_columns([\n    pl.Series('NO2_sat_adj_for_O3', apply_satellite_adjustment(df, sat_adjustments['O3_NO2_sat'])),\n    pl. Series('HCHO_sat_adj_for_O3', apply_satellite_adjustment(df, sat_adjustments['O3_HCHO_sat'])),\n    pl.Series('NO2_sat_adj_for_NO2', apply_satellite_adjustment(df, sat_adjustments['NO2_NO2_sat'])),\n])\n\n# Create adjusted satellite interaction features\ndf = df.with_columns([\n    (pl.col('NO2_sat_adj_for_O3') * pl.col('cos_sza')).alias('no2_sat_adj_radiation'),\n    (pl.col('HCHO_sat_adj_for_O3') * pl.col('cos_sza')).alias('hcho_sat_adj_radiation'),\n    (pl.col('NO2_sat_adj_for_O3') * pl. col('ventilation')).alias('no2_sat_adj_vent'),\n    (pl.col('HCHO_sat_adj_for_O3') / (pl.col('NO2_sat_adj_for_O3').clip(0.01, None))).alias('voc_nox_ratio_adj'),\n])\n\nprint(f\"  Satellite adjustment complete.  New columns added.\")\nprint(f\"  DataFrame shape: {df. shape[0]:,} x {df.shape[1]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:28:54.614522Z","iopub.execute_input":"2025-11-30T13:28:54.615149Z","iopub.status.idle":"2025-11-30T13:30:10.728295Z","shell.execute_reply.started":"2025-11-30T13:28:54.615098Z","shell.execute_reply":"2025-11-30T13:30:10.727604Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nSTAGE 3: SATELLITE DIURNAL ADJUSTMENT\n================================================================================\n\n  Adjusting NO2_satellite_filled for O3_target...\n    Raw satellite -> Correlation: -0.0683, RMSE: 45.10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    Training adjustment model:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"    Adjusted satellite -> Correlation: 0.8287, RMSE: 19.02\n    Improvement: Corr +0.8970, RMSE +26.07\n\n  Adjusting HCHO_satellite_filled for O3_target...\n    Raw satellite -> Correlation: 0.0624, RMSE: 44.50\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    Training adjustment model:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"    Adjusted satellite -> Correlation: 0.8251, RMSE: 19.20\n    Improvement: Corr +0.7627, RMSE +25.30\n\n  Adjusting NO2_satellite_filled for NO2_target...\n    Raw satellite -> Correlation: 0.3266, RMSE: 46.85\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    Training adjustment model:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"    Adjusted satellite -> Correlation: 0.8090, RMSE: 17.31\n    Improvement: Corr +0.4824, RMSE +29.55\n\n  Applying satellite adjustments to full dataset...\n  Satellite adjustment complete.  New columns added.\n  DataFrame shape: 171,679 x 127\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Cell 5: OOF Lag Creation - Iteration 1\nprint(\"\\n\" + \"=\" * 80)\nprint(\"STAGE 4: OOF LAG CREATION - ITERATION 1\")\nprint(\"=\" * 80)\n\ndef get_base_features(df, config):\n    \"\"\"Get all base features (excluding targets and identifiers).\"\"\"\n    exclude = [\n        'year', 'month', 'day', 'hour', 'date', 'datetime', 'doy',\n        'O3_target', 'NO2_target', 'site_id',\n        'NO2_satellite', 'HCHO_satellite', 'ratio_satellite',\n    ]\n    # Corrected spurious space in df.columns\n    # Also exclude any lag features from previous iterations\n    features = [c for c in df.columns if c not in exclude and '_lag_' not in c \n                and '_rmean_' not in c and '_rstd_' not in c and '_oof_iter' not in c]\n    return features\n\ndef prepare_arrays(df, features, target):\n    \"\"\"Prepare X, y arrays for training.\"\"\"\n    # Corrected spurious space in pl.col\n    df_valid = df.filter(pl.col(target).is_not_null())\n    \n    available = [f for f in features if f in df_valid.columns]\n    X = df_valid.select(available).to_numpy().astype(np.float32)\n    \n    # Corrected spurious space in np.float32\n    y = df_valid[target].to_numpy().astype(np.float32)\n    \n    # Corrected spurious space in np.where\n    # Median imputation\n    col_medians = np.nanmedian(X, axis=0)\n    col_medians = np.where(np.isnan(col_medians), 0, col_medians)\n    for j in range(X.shape[1]):\n        X[np.isnan(X[:, j]), j] = col_medians[j]\n    \n    return X, y, available, col_medians\n\ndef train_catboost_oof(X, y, config, desc=\"Training\"):\n    \"\"\"\n    Train CatBoost with K-fold CV and return OOF predictions.\n    Confidently using GPU and devices='0:1'.\n    \"\"\"\n    kf = KFold(n_splits=config['n_folds'], shuffle=True, random_state=config['random_state'])\n    \n    oof = np.zeros(len(y))\n    models = []\n    fold_metrics = []\n    \n    # Corrected spurious space in kf.split(X)\n    for fold_idx, (train_idx, val_idx) in enumerate(tqdm(kf.split(X), total=config['n_folds'],\n                                                          desc=desc, leave=False)):\n        model = cb.CatBoostRegressor(\n            iterations=config['iterations'],\n            depth=config['depth'],\n            learning_rate=config['learning_rate'],\n            l2_leaf_reg=5,\n            loss_function='RMSE',\n            random_seed=config['random_state'] + fold_idx,\n            verbose=False,\n            early_stopping_rounds=config['early_stopping'],\n            # Confidently using GPU with devices '0:1' as requested\n            task_type='GPU', \n            devices='0:1' \n        )\n        model.fit(X[train_idx], y[train_idx],\n                  eval_set=(X[val_idx], y[val_idx]), verbose=False)\n        \n        preds = model.predict(X[val_idx])\n        oof[val_idx] = preds\n        models.append(model)\n        \n        fold_rmse = np.sqrt(mean_squared_error(y[val_idx], preds))\n        fold_metrics.append(fold_rmse)\n    \n    avg_rmse = np.mean(fold_metrics)\n    std_rmse = np.std(fold_metrics)\n    \n    return oof, models, avg_rmse, std_rmse\n\ndef create_lag_features(df, oof_col, target_name, iteration):\n    \"\"\"Create lag and rolling features from OOF predictions.\"\"\"\n    df = df.sort(['site_id', 'datetime'])\n    \n    # Lag features\n    lags = [1, 3, 6, 12, 24, 48, 168]\n    for lag in lags:\n        df = df.with_columns([\n            pl.col(oof_col).shift(lag).over('site_id').alias(f'{target_name}_lag_{lag}h_iter_{iteration}')\n        ])\n    \n    # Rolling mean features (Corrected spurious space in pl.col)\n    for window in [6, 24, 168]:\n        df = df.with_columns([\n            pl.col(oof_col).rolling_mean(window_size=window, min_periods=1).over('site_id')\n            .alias(f'{target_name}_rmean_{window}h_iter_{iteration}')\n        ])\n    \n    # Rolling std features (Corrected spurious space in .alias)\n    for window in [6, 24, 168]:\n        df = df.with_columns([\n            pl.col(oof_col).rolling_std(window_size=window, min_periods=2).over('site_id')\n            .alias(f'{target_name}_rstd_{window}h_iter_{iteration}')\n        ])\n    \n    return df\n\n# Get base features\nbase_features = get_base_features(df, CONFIG)\nprint(f\"\\n  Base features for Iteration 1: {len(base_features)}\")\n\n# Train and create lags for each target\niteration_1_results = {}\n\nfor target in CONFIG['targets']:\n    print(f\"\\n  {target}:\")\n    print(\"-\" * 40)\n    \n    X, y, used_features, col_medians = prepare_arrays(df, base_features, target)\n    print(f\"    Samples: {len(y):,}, Features: {len(used_features)}\")\n    \n    # FUNCTION CALL: Using the corrected train_catboost_oof function\n    oof, models, avg_rmse, std_rmse = train_catboost_oof(X, y, CONFIG, desc=f\"    {target} Folds\")\n    print(f\"    OOF RMSE: {avg_rmse:.4f} (+/- {std_rmse:.4f})\")\n    \n    # Store OOF predictions\n    oof_col = f'{target}_oof_iter_0'\n    oof_full = np.full(len(df), np.nan)\n    \n    # Corrected spurious space in to_numpy()\n    valid_mask = df[target].is_not_null().to_numpy()\n    oof_full[valid_mask] = oof\n    \n    df = df.with_columns(pl.Series(oof_col, oof_full))\n    \n    # Create lag features\n    df = create_lag_features(df, oof_col, target, iteration=0)\n    \n    iteration_1_results[target] = {\n        'oof': oof,\n        'models': models,\n        'features': used_features,\n        'col_medians': col_medians,\n        'rmse': avg_rmse,\n        'std': std_rmse,\n    }\n\n# Corrected spurious space in df.shape\nprint(f\"\\n  Iteration 1 complete.\")\nprint(f\"  DataFrame shape: {df.shape[0]:,} x {df.shape[1]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:33:01.626464Z","iopub.execute_input":"2025-11-30T13:33:01.627083Z","iopub.status.idle":"2025-11-30T13:54:26.018835Z","shell.execute_reply.started":"2025-11-30T13:33:01.627054Z","shell.execute_reply":"2025-11-30T13:54:26.018026Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nSTAGE 4: OOF LAG CREATION - ITERATION 1\n================================================================================\n\n  Base features for Iteration 1: 102\n\n  O3_target:\n----------------------------------------\n    Samples: 171,679, Features: 102\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    O3_target Folds:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"    OOF RMSE: 11.9405 (+/- 0.0730)\n\n  NO2_target:\n----------------------------------------\n    Samples: 171,679, Features: 102\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    NO2_target Folds:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"    OOF RMSE: 10.4517 (+/- 0.0769)\n\n  Iteration 1 complete.\n  DataFrame shape: 171,679 x 155\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Cell 6: OOF Lag Creation - Iteration 2 (Final)\nprint(\"\\n\" + \"=\" * 80)\nprint(\"STAGE 5: OOF LAG CREATION - ITERATION 2 (FINAL)\")\nprint(\"=\" * 80)\n\ndef get_features_with_lags(df, target, iteration, config):\n    \"\"\"Get features including lag features from previous iteration.\"\"\"\n    exclude = [\n        'year', 'month', 'day', 'hour', 'date', 'datetime', 'doy',\n        'O3_target', 'NO2_target', 'site_id',\n        'NO2_satellite', 'HCHO_satellite', 'ratio_satellite',\n    ]\n    \n    # Get all base features\n    # Corrected spurious space\n    base = [c for c in df.columns if c not in exclude \n            and '_oof_iter' not in c]  # Exclude OOF columns themselves\n    \n    # Filter to only include lags from correct iteration and target\n    features = []\n    for f in base:\n        # Include non-lag features\n        if '_lag_' not in f and '_rmean_' not in f and '_rstd_' not in f:\n            # Corrected spurious space\n            features.append(f)\n        # Include lags from previous iteration for THIS target only\n        elif f'_iter_{iteration-1}' in f and target.replace('_target', '') in f:\n            # Corrected spurious space\n            features.append(f)\n    \n    return list(dict.fromkeys(features))  # Remove duplicates\n\n# Train Iteration 2 for each target\niteration_2_results = {}\n\nfor target in CONFIG['targets']:\n    print(f\"\\n  {target}:\")\n    print(\"-\" * 40)\n    \n    # Get features including Iteration 0 lags\n    # FUNCTION CALL: Uses get_features_with_lags\n    features = get_features_with_lags(df, target, iteration=1, config=CONFIG)\n    \n    # Count lag features\n    lag_features = [f for f in features if '_iter_0' in f]\n    print(f\"    Base features: {len(features) - len(lag_features)}, Lag features: {len(lag_features)}\")\n    \n    # FUNCTION CALL: prepare_arrays (assumes definition from Cell 5)\n    X, y, used_features, col_medians = prepare_arrays(df, features, target)\n    print(f\"    Total features: {len(used_features)}, Samples: {len(y):,}\")\n    \n    # FUNCTION CALL: train_catboost_oof (assumes definition from Cell 5, uses GPU configuration)\n    oof, models, avg_rmse, std_rmse = train_catboost_oof(X, y, CONFIG, desc=f\"    {target} Folds\")\n    \n    # Compare with Iteration 1\n    iter1_rmse = iteration_1_results[target]['rmse']\n    improvement = iter1_rmse - avg_rmse\n    print(f\"    OOF RMSE: {avg_rmse:.4f} (+/- {std_rmse:.4f})\")\n    print(f\"    Improvement from Iter 1: {improvement:+.4f} ({improvement/iter1_rmse*100:+.1f}%)\")\n    \n    # Store OOF predictions\n    oof_col = f'{target}_oof_iter_1'\n    # Corrected spurious space in np.full\n    oof_full = np.full(len(df), np.nan)\n    # Corrected spurious space in is_not_null()\n    valid_mask = df[target].is_not_null().to_numpy()\n    oof_full[valid_mask] = oof\n    \n    df = df.with_columns(pl.Series(oof_col, oof_full))\n    \n    # Create final lag features\n    # FUNCTION CALL: create_lag_features (assumes definition from Cell 5)\n    df = create_lag_features(df, oof_col, target, iteration=1)\n    \n    iteration_2_results[target] = {\n        'oof': oof,\n        'models': models,\n        'features': used_features,\n        'col_medians': col_medians,\n        'rmse': avg_rmse,\n        'std': std_rmse,\n    }\n\n# Corrected spurious space in df.shape\nprint(f\"\\n  Iteration 2 complete.\")\nprint(f\"  DataFrame shape: {df.shape[0]:,} x {df.shape[1]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T13:59:47.475735Z","iopub.execute_input":"2025-11-30T13:59:47.476307Z","iopub.status.idle":"2025-11-30T14:23:02.562996Z","shell.execute_reply.started":"2025-11-30T13:59:47.476280Z","shell.execute_reply":"2025-11-30T14:23:02.562403Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nSTAGE 5: OOF LAG CREATION - ITERATION 2 (FINAL)\n================================================================================\n\n  O3_target:\n----------------------------------------\n    Base features: 102, Lag features: 13\n    Total features: 115, Samples: 171,679\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    O3_target Folds:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"    OOF RMSE: 10.2105 (+/- 0.0688)\n    Improvement from Iter 1: +1.7300 (+14.5%)\n\n  NO2_target:\n----------------------------------------\n    Base features: 102, Lag features: 13\n    Total features: 115, Samples: 171,679\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    NO2_target Folds:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"    OOF RMSE: 9.1549 (+/- 0.1112)\n    Improvement from Iter 1: +1.2968 (+12.4%)\n\n  Iteration 2 complete.\n  DataFrame shape: 171,679 x 183\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Cell 1: Additional OOF Iterations (3 more: iterations 2, 3, 4)\nprint(\"=\" * 80)\nprint(\"STAGE 7: ADDITIONAL OOF ITERATIONS (3 MORE)\")\nprint(\"=\" * 80)\n\ndef get_features_for_iteration(df, target, current_iter, config):\n    \"\"\"Get features including lags from previous iteration.\"\"\"\n    exclude = [\n        'year', 'month', 'day', 'hour', 'date', 'datetime', 'doy',\n        'O3_target', 'NO2_target', 'site_id',\n        'NO2_satellite', 'HCHO_satellite', 'ratio_satellite',\n    ]\n    \n    target_prefix = target.replace('_target', '')\n    features = []\n    \n    for c in df.columns:\n        if c in exclude:\n            continue\n        if '_oof_iter' in c:\n            continue\n        \n        # For lag features, only keep from previous iteration for this target\n        if '_iter_' in c:\n            iter_num = int(c.split('_iter_')[-1])\n            if iter_num == current_iter - 1 and target_prefix in c:\n                features.append(c)\n        else:\n            features.append(c)\n    \n    return list(dict.fromkeys(features))\n\ndef prepare_arrays_v2(df, features, target):\n    \"\"\"Prepare X, y arrays for training.\"\"\"\n    # Corrected spurious spaces\n    df_valid = df.filter(pl.col(target).is_not_null())\n    \n    available = [f for f in features if f in df_valid.columns]\n    X = df_valid.select(available).to_numpy().astype(np.float32)\n    y = df_valid[target].to_numpy().astype(np.float32)\n    \n    # Median imputation (corrected spurious spaces)\n    col_medians = np.nanmedian(X, axis=0)\n    col_medians = np.where(np.isnan(col_medians), 0, col_medians)\n    for j in range(X.shape[1]):\n        X[np.isnan(X[:, j]), j] = col_medians[j]\n    \n    return X, y, available, col_medians\n\ndef train_catboost_oof_v2(X, y, config, desc=\"Training\"):\n    \"\"\"\n    Train CatBoost with K-fold CV and return OOF predictions.\n    Uses GPU: devices='0:1' as requested.\n    \"\"\"\n    kf = KFold(n_splits=config['n_folds'], shuffle=True, random_state=config['random_state'])\n    \n    oof = np.zeros(len(y))\n    models = []\n    fold_metrics = []\n    \n    # Corrected spurious space\n    pbar = tqdm(enumerate(kf.split(X)), total=config['n_folds'], desc=desc, leave=False)\n    \n    for fold_idx, (train_idx, val_idx) in pbar:\n        model = cb.CatBoostRegressor(\n            iterations=config['iterations'],\n            depth=config['depth'],\n            learning_rate=config['learning_rate'],\n            l2_leaf_reg=5,\n            loss_function='RMSE',\n            random_seed=config['random_state'] + fold_idx,\n            verbose=False,\n            early_stopping_rounds=config['early_stopping'],\n            # Confidently using GPU configuration as requested\n            task_type='GPU',\n            devices='0:1' \n        )\n        model.fit(X[train_idx], y[train_idx],\n                  eval_set=(X[val_idx], y[val_idx]), verbose=False)\n        \n        preds = model.predict(X[val_idx])\n        oof[val_idx] = preds\n        models.append(model)\n        \n        fold_rmse = np.sqrt(mean_squared_error(y[val_idx], preds))\n        # Corrected spurious space\n        fold_metrics.append(fold_rmse)\n        pbar.set_postfix({'fold_rmse': f'{fold_rmse:.4f}'})\n    \n    avg_rmse = np.mean(fold_metrics)\n    std_rmse = np.std(fold_metrics)\n    \n    return oof, models, avg_rmse, std_rmse\n\ndef create_lag_features_v2(df, oof_col, target_name, iteration):\n    \"\"\"Create comprehensive lag and rolling features from OOF predictions.\"\"\"\n    df = df.sort(['site_id', 'datetime'])\n    \n    # Clock-time lags\n    lags = [1, 2, 3, 6, 12, 24, 48, 72, 168]\n    for lag in lags:\n        col_name = f'{target_name}_lag_{lag}h_iter_{iteration}'\n        if col_name not in df.columns:\n            df = df.with_columns([\n                # Corrected spurious space in .alias\n                pl.col(oof_col).shift(lag).over('site_id').alias(col_name)\n            ])\n    \n    # Rolling means\n    for window in [3, 6, 12, 24, 48, 168]:\n        col_name = f'{target_name}_rmean_{window}h_iter_{iteration}'\n        if col_name not in df.columns:\n            df = df.with_columns([\n                # Corrected spurious space in pl.col and .over\n                pl.col(oof_col).rolling_mean(window_size=window, min_periods=1).over('site_id')\n                .alias(col_name)\n            ])\n    \n    # Rolling stds\n    for window in [3, 6, 12, 24, 48, 168]:\n        col_name = f'{target_name}_rstd_{window}h_iter_{iteration}'\n        if col_name not in df.columns:\n            df = df.with_columns([\n                # Corrected spurious space in .alias\n                pl.col(oof_col).rolling_std(window_size=window, min_periods=2).over('site_id')\n                .alias(col_name)\n            ])\n    \n    # Rolling min/max\n    for window in [6, 24]:\n        min_name = f'{target_name}_rmin_{window}h_iter_{iteration}'\n        max_name = f'{target_name}_rmax_{window}h_iter_{iteration}'\n        if min_name not in df.columns:\n            df = df.with_columns([\n                # Corrected spurious space in .over and .alias\n                pl.col(oof_col).rolling_min(window_size=window, min_periods=1).over('site_id')\n                .alias(min_name),\n                pl.col(oof_col).rolling_max(window_size=window, min_periods=1).over('site_id')\n                .alias(max_name),\n            ])\n    \n    # Range (max - min)\n    for window in [6, 24]:\n        range_name = f'{target_name}_range_{window}h_iter_{iteration}'\n        if range_name not in df.columns:\n            df = df.with_columns([\n                # Corrected spurious space in inner pl.col\n                (pl.col(f'{target_name}_rmax_{window}h_iter_{iteration}') - \n                 pl.col(f'{target_name}_rmin_{window}h_iter_{iteration}')).alias(range_name)\n            ])\n    \n    return df\n\n# Store iteration results\n# NOTE: Assumes iteration_1_results and iteration_2_results were defined in preceding cells\nall_iteration_results = {\n    'O3_target': [iteration_1_results['O3_target'], iteration_2_results['O3_target']],\n    'NO2_target': [iteration_1_results['NO2_target'], iteration_2_results['NO2_target']],\n}\n\n# Run 3 more iterations (iterations 2, 3, 4 in 0-indexed)\nadditional_iterations = 3\n\nfor iter_idx in tqdm(range(2, 2 + additional_iterations), desc=\"OOF Iterations\"):\n    print(f\"\\n{'='*60}\")\n    print(f\"ITERATION {iter_idx + 1} (0-indexed: {iter_idx})\")\n    print(f\"{'='*60}\")\n    \n    for target in CONFIG['targets']:\n        print(f\"\\n  {target}:\")\n        print(\"-\" * 40)\n        \n        # Get features with previous iteration lags\n        features = get_features_for_iteration(df, target, iter_idx, CONFIG)\n        \n        # Count lag features\n        lag_feats = [f for f in features if f'_iter_{iter_idx-1}' in f]\n        base_feats = len(features) - len(lag_feats)\n        print(f\"    Base features: {base_feats}, Lag features: {len(lag_feats)}\")\n        \n        # FUNCTION CALL: prepare_arrays_v2\n        X, y, used_features, col_medians = prepare_arrays_v2(df, features, target)\n        print(f\"    Total features: {len(used_features)}, Samples: {len(y):,}\")\n        \n        # FUNCTION CALL: train_catboost_oof_v2\n        oof, models, avg_rmse, std_rmse = train_catboost_oof_v2(\n            X, y, CONFIG, desc=f\"    {target} Iter {iter_idx+1}\"\n        )\n        \n        # Compare with previous iteration\n        prev_rmse = all_iteration_results[target][-1]['rmse']\n        improvement = prev_rmse - avg_rmse\n        print(f\"    OOF RMSE: {avg_rmse:.4f} (+/- {std_rmse:.4f})\")\n        print(f\"    Improvement: {improvement:+.4f} ({improvement/prev_rmse*100:+.1f}%)\")\n        \n        # Store OOF predictions\n        oof_col = f'{target}_oof_iter_{iter_idx}'\n        oof_full = np.full(len(df), np.nan)\n        # Corrected spurious space\n        valid_mask = df[target].is_not_null().to_numpy()\n        oof_full[valid_mask] = oof\n        \n        df = df.with_columns(pl.Series(oof_col, oof_full))\n        \n        # Create lag features\n        df = create_lag_features_v2(df, oof_col, target, iteration=iter_idx)\n        \n        # Store results\n        all_iteration_results[target].append({\n            'oof': oof,\n            'models': models,\n            'features': used_features,\n            'col_medians': col_medians,\n            'rmse': avg_rmse,\n            'std': std_rmse,\n            'iteration': iter_idx,\n        })\n\n# Summary\nprint(\"\\n\" + \"=\" * 80)\nprint(\"OOF ITERATION SUMMARY\")\nprint(\"=\" * 80)\n\nprint(f\"\\n{'Target':<15} {'Iter 1':>10} {'Iter 2':>10} {'Iter 3':>10} {'Iter 4':>10} {'Iter 5':>10} {'Total Gain':>12}\")\nprint(\"-\" * 80)\n\nfor target in CONFIG['targets']:\n    results = all_iteration_results[target]\n    rmses = [r['rmse'] for r in results]\n    total_gain = rmses[0] - rmses[-1]\n    \n    row = f\"{target:<15}\"\n    for rmse in rmses:\n        row += f\" {rmse:>10.4f}\"\n    row += f\" {total_gain:>+12.4f}\"\n    print(row)\n\n# Corrected spurious space\nprint(f\"\\nDataFrame shape: {df.shape[0]:,} x {df.shape[1]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T14:23:31.206311Z","iopub.execute_input":"2025-11-30T14:23:31.207076Z","iopub.status.idle":"2025-11-30T15:43:34.571412Z","shell.execute_reply.started":"2025-11-30T14:23:31.207051Z","shell.execute_reply":"2025-11-30T15:43:34.570539Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nSTAGE 7: ADDITIONAL OOF ITERATIONS (3 MORE)\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"OOF Iterations:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"522c47135fae4b5cb6825e4e7c69e2d2"}},"metadata":{}},{"name":"stdout","text":"\n============================================================\nITERATION 3 (0-indexed: 2)\n============================================================\n\n  O3_target:\n----------------------------------------\n    Base features: 115, Lag features: 13\n    Total features: 128, Samples: 171,679\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    O3_target Iter 3:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"    OOF RMSE: 9.9642 (+/- 0.0993)\n    Improvement: +0.2463 (+2.4%)\n\n  NO2_target:\n----------------------------------------\n    Base features: 115, Lag features: 13\n    Total features: 128, Samples: 171,679\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    NO2_target Iter 3:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"    OOF RMSE: 8.9722 (+/- 0.1439)\n    Improvement: +0.1827 (+2.0%)\n\n============================================================\nITERATION 4 (0-indexed: 3)\n============================================================\n\n  O3_target:\n----------------------------------------\n    Base features: 115, Lag features: 27\n    Total features: 142, Samples: 171,679\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    O3_target Iter 4:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"    OOF RMSE: 9.6812 (+/- 0.0952)\n    Improvement: +0.2830 (+2.8%)\n\n  NO2_target:\n----------------------------------------\n    Base features: 115, Lag features: 27\n    Total features: 142, Samples: 171,679\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    NO2_target Iter 4:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"    OOF RMSE: 8.7063 (+/- 0.1599)\n    Improvement: +0.2659 (+3.0%)\n\n============================================================\nITERATION 5 (0-indexed: 4)\n============================================================\n\n  O3_target:\n----------------------------------------\n    Base features: 115, Lag features: 27\n    Total features: 142, Samples: 171,679\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    O3_target Iter 5:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"    OOF RMSE: 9.6287 (+/- 0.0865)\n    Improvement: +0.0524 (+0.5%)\n\n  NO2_target:\n----------------------------------------\n    Base features: 115, Lag features: 27\n    Total features: 142, Samples: 171,679\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    NO2_target Iter 5:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"    OOF RMSE: 8.6759 (+/- 0.1485)\n    Improvement: +0.0304 (+0.3%)\n\n================================================================================\nOOF ITERATION SUMMARY\n================================================================================\n\nTarget              Iter 1     Iter 2     Iter 3     Iter 4     Iter 5   Total Gain\n--------------------------------------------------------------------------------\nO3_target          11.9405    10.2105     9.9642     9.6812     9.6287      +2.3118\nNO2_target         10.4517     9.1549     8.9722     8.7063     8.6759      +1.7759\n\nDataFrame shape: 171,679 x 351\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Assuming 'pl' (polars), 'np' (numpy), 'additional_iterations', 'df', and 'CONFIG' are imported/defined.\n\n# Cell 2: Advanced Feature Engineering\nprint(\"\\n\" + \"=\" * 80)\nprint(\"STAGE 8: ADVANCED FEATURE ENGINEERING\")\nprint(\"=\" * 80)\n\ndef create_advanced_features(df, config, last_iter):\n    \"\"\"Create advanced features for final model.\"\"\"\n\n    # FIX 1: Correct Polars sort syntax: pass column names as positional arguments\n    df = df.sort('site_id', 'datetime')\n    initial_cols = len(df.columns)\n\n    # =========================================================================\n    # 1. CROSS-TARGET FEATURES (O3-NO2 interactions)\n    # =========================================================================\n    print(\"\\n  [1/7] Creating cross-target features...\")\n\n    # O3-NO2 ratio from OOF predictions\n    o3_oof = f'O3_target_oof_iter_{last_iter}'\n    no2_oof = f'NO2_target_oof_iter_{last_iter}'\n\n    if o3_oof in df.columns and no2_oof in df.columns:\n        # FIXES 2-7 from previous round: Remove spaces within pl.col(), .clip(), and .alias()\n        df = df.with_columns([\n            (pl.col(o3_oof) / (pl.col(no2_oof).clip(0.1, None))).alias('oof_o3_no2_ratio'),\n            (pl.col(o3_oof) - pl.col(no2_oof)).alias('oof_o3_no2_diff'),\n            (pl.col(o3_oof) + pl.col(no2_oof)).alias('oof_ox_total'),  # Ox = O3 + NO2\n        ])\n\n    # =========================================================================\n    # 2. LAG DIFFERENCES (momentum features)\n    # =========================================================================\n    print(\"  [2/7] Creating lag difference features...\")\n\n    for target in config['targets']:\n        # Difference between consecutive lags (momentum)\n        lag_1h = f'{target}_lag_1h_iter_{last_iter}'\n        lag_3h = f'{target}_lag_3h_iter_{last_iter}'\n        lag_6h = f'{target}_lag_6h_iter_{last_iter}'\n        lag_24h = f'{target}_lag_24h_iter_{last_iter}'\n\n        if lag_1h in df.columns and lag_3h in df.columns:\n            # FIXES 8-10 from previous round: Remove spaces within pl.col() and .alias()\n            df = df.with_columns([\n                (pl.col(lag_1h) - pl.col(lag_3h)).alias(f'{target}_diff_1h_3h'),\n            ])\n\n        if lag_1h in df.columns and lag_6h in df.columns:\n            # FIXES 11-13 from previous round: Remove spaces within pl.col() and .alias()\n            df = df.with_columns([\n                (pl.col(lag_1h) - pl.col(lag_6h)).alias(f'{target}_diff_1h_6h'),\n            ])\n\n        if lag_1h in df.columns and lag_24h in df.columns:\n            # FIX 14 from previous round: Remove space in df. columns\n            df = df.with_columns([\n                (pl.col(lag_1h) - pl.col(lag_24h)).alias(f'{target}_diff_1h_24h'),\n            ])\n\n        if lag_3h in df.columns and lag_6h in df.columns:\n            # FIX 15 from previous round: Remove space in .alias()\n            df = df.with_columns([\n                (pl.col(lag_3h) - pl.col(lag_6h)).alias(f'{target}_diff_3h_6h'),\n            ])\n\n        # Rate of change\n        if lag_1h in df.columns and lag_3h in df.columns:\n            # FIX 38 (The current error): Added opening parenthesis before (pl.col(...)\n            df = df.with_columns([\n                ((pl.col(lag_1h) - pl.col(lag_3h)) / 2).alias(f'{target}_rate_3h'),\n            ])\n\n        if lag_1h in df.columns and lag_6h in df.columns:\n            # FIX 16 from previous round: Remove space in df. columns\n            df = df.with_columns([\n                ((pl.col(lag_1h) - pl.col(lag_6h)) / 5).alias(f'{target}_rate_6h'),\n            ])\n\n    # =========================================================================\n    # 3. VOLATILITY RATIOS\n    # =========================================================================\n    print(\"  [3/7] Creating volatility ratio features...\")\n\n    for target in config['targets']:\n        rstd_6h = f'{target}_rstd_6h_iter_{last_iter}'\n        rstd_24h = f'{target}_rstd_24h_iter_{last_iter}'\n        rmean_6h = f'{target}_rmean_6h_iter_{last_iter}'\n        rmean_24h = f'{target}_rmean_24h_iter_{last_iter}'\n\n        # Coefficient of variation (normalized volatility)\n        if rstd_6h in df.columns and rmean_6h in df.columns:\n            # FIXES 17-19 from previous round: Remove spaces in df. with_columns, .clip(), and .alias()\n            df = df.with_columns([\n                (pl.col(rstd_6h) / (pl.col(rmean_6h).clip(1.0, None))).alias(f'{target}_cv_6h'),\n            ])\n\n        if rstd_24h in df.columns and rmean_24h in df.columns:\n            df = df.with_columns([\n                (pl.col(rstd_24h) / (pl.col(rmean_24h).clip(1.0, None))).alias(f'{target}_cv_24h'),\n            ])\n\n        # Volatility ratio (short vs long term)\n        if rstd_6h in df.columns and rstd_24h in df.columns:\n            # FIXES 20-21 from previous round: Remove spaces in pl. col() and .alias()\n            df = df.with_columns([\n                (pl.col(rstd_6h) / (pl.col(rstd_24h).clip(0.1, None))).alias(f'{target}_vol_ratio'),\n            ])\n\n    # =========================================================================\n    # 4. TIME-BASED INTERACTIONS\n    # =========================================================================\n    print(\"  [4/7] Creating time-based interaction features...\")\n\n    # Hour-specific features\n    df = df.with_columns([\n        (pl.col('hour_sin') * pl.col('is_rush_hour')).alias('hour_rush_interaction'),\n        (pl.col('cos_sza') * pl.col('is_weekend')).alias('sza_weekend_interaction'),\n    ])\n\n    if 'is_daytime' in df.columns:\n        df = df.with_columns([\n            (pl.col('hour_cos') * pl.col('is_daytime')).alias('hour_daytime_interaction'),\n        ])\n\n    # Hour of week (0-167)\n    df = df.with_columns([\n        # FIX 22 from previous round: Remove space in pl. col('dayofweek')\n        (pl.col('dayofweek') * 24 + pl.col('hour')).alias('hour_of_week'),\n    ])\n\n    df = df.with_columns([\n        (pl.col('hour_of_week') * 2 * np.pi / 168).sin().alias('hour_of_week_sin'),\n        # FIX 23 from previous round: Remove spaces in pl. col('hour_of_week') * 2 * np. pi / 168). cos()\n        (pl.col('hour_of_week') * 2 * np.pi / 168).cos().alias('hour_of_week_cos'),\n    ])\n\n    # =========================================================================\n    # 5. METEOROLOGICAL INTERACTIONS WITH LAGS\n    # =========================================================================\n    print(\"  [5/7] Creating met-lag interaction features...\")\n\n    for target in config['targets']:\n        lag_1h = f'{target}_lag_1h_iter_{last_iter}'\n\n        if lag_1h in df.columns:\n            if 'wind_speed' in df.columns:\n                df = df.with_columns([\n                    (pl.col(lag_1h) * pl.col('wind_speed')).alias(f'{target}_lag1h_wind'),\n                ])\n\n            if 'ventilation' in df.columns:\n                # FIX 24 from previous round: Remove space in df. columns\n                df = df.with_columns([\n                    (pl.col(lag_1h) * pl.col('ventilation')).alias(f'{target}_lag1h_vent'),\n                ])\n\n            if 'T_forecast' in df.columns:\n                df = df.with_columns([\n                    (pl.col(lag_1h) * pl.col('T_forecast')).alias(f'{target}_lag1h_temp'),\n                ])\n\n            if 'is_stagnant' in df.columns:\n                # FIX 25 from previous round: Remove space in df. columns\n                df = df.with_columns([\n                    (pl.col(lag_1h) * pl.col('is_stagnant')).alias(f'{target}_lag1h_stagnant'),\n                ])\n\n    # =========================================================================\n    # 6. DIURNAL POSITION FEATURES\n    # =========================================================================\n    print(\"  [6/7] Creating diurnal position features...\")\n\n    # Hours since sunrise/sunset proxy\n    df = df.with_columns([\n        # FIXES 26-27 from previous round: Remove spaces in pl. col('cos_sza') and pl. col('hour')\n        pl.when(pl.col('cos_sza') > 0.1)\n        .then(pl.col('hour') - 6)\n        .otherwise(0)\n        .alias('hours_since_sunrise'),\n    ])\n\n    df = df.with_columns([\n        # FIXES 28-29 from previous round: Remove spaces in pl. col('cos_sza') and pl.col('hour')\n        pl.when(pl.col('cos_sza') <= 0.1)\n        .then(pl.col('hour') - 18)\n        .otherwise(0)\n        .alias('hours_since_sunset'),\n    ])\n\n    # Peak hour indicators\n    df = df.with_columns([\n        # FIXES 30-35 from previous round: Remove spaces in pl. col('hour') and pl. Int8 and .alias()\n        ((pl.col('hour') >= 12) & (pl.col('hour') <= 16)).cast(pl.Int8).alias('is_o3_peak_hours'),\n        ((pl.col('hour') >= 7) & (pl.col('hour') <= 9)).cast(pl.Int8).alias('is_no2_morning_peak'),\n        ((pl.col('hour') >= 18) & (pl.col('hour') <= 21)).cast(pl.Int8).alias('is_no2_evening_peak'),\n    ])\n\n    # =========================================================================\n    # 7. SITE-TEMPORAL INTERACTIONS\n    # =========================================================================\n    print(\"  [7/7] Creating site-temporal features...\")\n\n    # Site-hour interaction (captures site-specific diurnal patterns)\n    for site_id in range(1, config['n_sites'] + 1):\n        site_col = f'site_{site_id}'\n        if site_col in df.columns:\n            df = df.with_columns([\n                (pl.col(site_col) * pl.col('hour_sin')).alias(f'site{site_id}_hour_sin'),\n                (pl.col(site_col) * pl.col('is_rush_hour')).alias(f'site{site_id}_rush'),\n            ])\n\n    new_cols = len(df.columns) - initial_cols\n    print(f\"\\n  Created {new_cols} new features\")\n    print(f\"  Total columns: {len(df.columns)}\")\n\n    return df\n\n# Determine last iteration index\n# Assuming 'additional_iterations' is defined globally or earlier (e.g., additional_iterations = 3)\nlast_iter = 2 + additional_iterations - 1  # Should be 4 (0-indexed)\n\n# Create advanced features\ndf = create_advanced_features(df, CONFIG, last_iter)\n\n# Get final feature list for each target\ndef get_final_feature_list(df, target, config, last_iter):\n    \"\"\"Get comprehensive final feature list.\"\"\"\n    exclude = [\n        'year', 'month', 'day', 'hour', 'date', 'datetime', 'doy',\n        'O3_target', 'NO2_target', 'site_id',\n        'NO2_satellite', 'HCHO_satellite', 'ratio_satellite',\n        'hour_of_week',  # Keep encoded version only\n    ]\n\n    target_prefix = target.replace('_target', '')\n    other_target = 'NO2' if 'O3' in target else 'O3'\n\n    features = []\n\n    for c in df.columns:\n        if c in exclude:\n            continue\n        if '_oof_iter' in c:\n            continue\n\n        # Skip lag features from other target\n        if f'{other_target}_target_lag_' in c:\n            continue\n        if f'{other_target}_target_rmean_' in c:\n            continue\n        if f'{other_target}_target_rstd_' in c:\n            continue\n        if f'{other_target}_target_rmin_' in c:\n            continue\n        if f'{other_target}_target_rmax_' in c:\n            continue\n        if f'{other_target}_target_range_' in c:\n            continue\n        if f'{other_target}_target_diff_' in c:\n            continue\n        if f'{other_target}_target_rate_' in c:\n            continue\n        if f'{other_target}_target_cv_' in c:\n            continue\n        if f'{other_target}_target_vol_' in c:\n            continue\n        if f'{other_target}_target_lag1h_' in c:\n            continue\n\n        # For lag features, only keep from last iteration for this target\n        if '_iter_' in c and target_prefix in c:\n            iter_num = int(c.split('_iter_')[-1])\n            if iter_num == last_iter:\n                features.append(c)\n        elif '_iter_' not in c:\n            features.append(c)\n\n    return list(dict.fromkeys(features))\n\n# Prepare final features\nfinal_features = {}\nfor target in CONFIG['targets']:\n    final_features[target] = get_final_feature_list(df, target, CONFIG, last_iter)\n    print(f\"\\n{target}: {len(final_features[target])} final features\")\n\n    # Show feature categories\n    lag_feats = [f for f in final_features[target] if '_lag_' in f or '_rmean_' in f or '_rstd_' in f]\n    # FIX 36 from previous round: Remove space in f. lower()\n    ctm_feats = [f for f in final_features[target] if 'forecast' in f.lower() or 'ctm' in f.lower() or 'corr' in f.lower()]\n    sat_feats = [f for f in final_features[target] if 'sat' in f.lower()]\n\n    print(f\"  - Lag/Rolling: {len(lag_feats)}\")\n    print(f\"  - CTM: {len(ctm_feats)}\")\n    print(f\"  - Satellite: {len(sat_feats)}\")\n    print(f\"  - Other: {len(final_features[target]) - len(lag_feats) - len(ctm_feats) - len(sat_feats)}\")\n\n# FIX 37 from previous round: Remove space in df. shape\nprint(f\"\\nDataFrame shape: {df.shape[0]:,} x {df.shape[1]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T15:50:44.657362Z","iopub.execute_input":"2025-11-30T15:50:44.658101Z","iopub.status.idle":"2025-11-30T15:50:44.804481Z","shell.execute_reply.started":"2025-11-30T15:50:44.658073Z","shell.execute_reply":"2025-11-30T15:50:44.803692Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nSTAGE 8: ADVANCED FEATURE ENGINEERING\n================================================================================\n\n  [1/7] Creating cross-target features...\n  [2/7] Creating lag difference features...\n  [3/7] Creating volatility ratio features...\n  [4/7] Creating time-based interaction features...\n  [5/7] Creating met-lag interaction features...\n  [6/7] Creating diurnal position features...\n  [7/7] Creating site-temporal features...\n\n  Created 44 new features\n  Total columns: 395\n\nO3_target: 172 final features\n  - Lag/Rolling: 34\n  - CTM: 19\n  - Satellite: 14\n  - Other: 105\n\nNO2_target: 172 final features\n  - Lag/Rolling: 34\n  - CTM: 19\n  - Satellite: 14\n  - Other: 105\n\nDataFrame shape: 171,679 x 395\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Assuming all necessary imports (Polars, NumPy, KFold, CatBoost, tqdm, Optuna, train_test_split) are present.\nfrom sklearn.model_selection import train_test_split\n\n# --- Configuration Setup ---\nif 'random_state' not in CONFIG:\n    CONFIG['random_state'] = 42\nif 'n_folds' not in CONFIG:\n    CONFIG['n_folds'] = 5\n\nN_TRIALS = 20\nRANDOM_STATE = CONFIG.get('random_state', 42)\nN_FOLDS = CONFIG.get('n_folds', 5)\n\n# Cell 3: Hyperparameter Tuning with Optuna (80/20 Split Version)\nprint(\"\\n\" + \"=\" * 80)\nprint(\"STAGE 9: HYPERPARAMETER TUNING WITH OPTUNA (80/20 SPLIT)\")\nprint(\"=\" * 80)\n\nimport optuna\nfrom optuna.samplers import TPESampler\n# We use WARNING to keep the output clean, letting the TQDM bar show overall progress\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\n# --- Data Preparation ---\ndef prepare_arrays_v3(df, features, target):\n    \"\"\"Prepare X, y arrays for training.\"\"\"\n    df_valid = df.filter(pl.col(target).is_not_null())\n    available = [f for f in features if f in df_valid.columns]\n    X = df_valid.select(available).to_numpy().astype(np.float32)\n    y = df_valid[target].to_numpy().astype(np.float32)\n    \n    # Median imputation\n    col_medians = np.nanmedian(X, axis=0)\n    col_medians = np.where(np.isnan(col_medians), 0, col_medians)\n    for j in range(X.shape[1]):\n        X[np.isnan(X[:, j]), j] = col_medians[j]\n    \n    return X, y, available, col_medians\n\n# --- Objective Factory (Modified for 80/20 Split) ---\ndef objective_factory(X, y, random_state):\n    \"\"\"\n    Create objective function for Optuna using a single 80/20 split.\n    This is 5x faster than 5-fold CV for tuning.\n    \"\"\"\n    \n    # 1. Perform the split ONCE here, outside the trial loop\n    #    This ensures every trial uses the exact same validation set for fair comparison.\n    X_train, X_val, y_train, y_val = train_test_split(\n        X, y, test_size=0.20, random_state=random_state, shuffle=True\n    )\n    \n    def objective(trial):\n        params = {\n            'iterations': trial.suggest_int('iterations', 800,1000),\n            'depth': trial.suggest_int('depth', 12, 14),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n            'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n            'bootstrap_type': 'Bernoulli',\n            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n            'loss_function': 'RMSE',\n            'random_seed': random_state,\n            'verbose': False,\n            'early_stopping_rounds': 100,\n            'task_type': 'GPU',\n            'devices': '0:1', \n        }\n        \n        # 2. Train a single model on the split\n        model = cb.CatBoostRegressor(**params)\n        \n        model.fit(\n            X_train, y_train,\n            eval_set=(X_val, y_val),\n            verbose=False\n        )\n        \n        preds = model.predict(X_val)\n        rmse = np.sqrt(mean_squared_error(y_val, preds))\n        \n        return rmse\n    \n    return objective\n\n# --- Tuning Execution ---\n\ntuning_results = {}\n\nfor target in tqdm(CONFIG['targets'], desc=\"Tuning Targets\"):\n    print(f\"\\n{'='*60}\")\n    print(f\"TUNING {target}\")\n    print(f\"{'='*60}\")\n    \n    # Prepare data\n    features = final_features[target]\n    X, y, used_features, col_medians = prepare_arrays_v3(df, features, target)\n    \n    print(f\"  Features: {len(used_features)}, Samples: {len(y):,}\")\n    \n    # Create study\n    sampler = TPESampler(seed=RANDOM_STATE)\n    study = optuna.create_study(direction='minimize', sampler=sampler)\n    \n    # Create objective (Using 80/20 split factory)\n    objective = objective_factory(X, y, RANDOM_STATE)\n    \n    # Optimize with progress tracking\n    print(f\"  Running {N_TRIALS} trials (80/20 Holdout)...\")\n    \n    with tqdm(total=N_TRIALS, desc=f\"  {target} Trials\", leave=True) as pbar_instance:\n        def minimal_callback(study, trial):\n            if trial.state.is_finished():\n                pbar_instance.update(1)\n                pbar_instance.set_postfix({'best_rmse': f\"{study.best_value:.4f}\"})\n\n        study.optimize(objective, n_trials=N_TRIALS, callbacks=[minimal_callback], show_progress_bar=False)\n    \n    best_params = study.best_params\n    best_rmse = study.best_value\n    \n    print(f\"\\n  Best RMSE (Val): {best_rmse:.4f} (Trial {study.best_trial.number})\")\n    print(f\"  Best Parameters:\")\n    for k, v in best_params.items():\n        if isinstance(v, float):\n            print(f\"    {k}: {v:.6f}\")\n        else:\n            print(f\"    {k}: {v}\")\n    \n    # --- FINAL TRAINING (Still using 5-Fold CV) ---\n    # We use 5-fold here to generate the robust OOF predictions needed for later stages.\n    print(f\"\\n  Training final model with best params (5-Fold CV)...\")\n    \n    final_params = {\n        **best_params, \n        'loss_function': 'RMSE',\n        'verbose': False,\n        'early_stopping_rounds': 100,\n        'task_type': 'GPU', \n        'devices': '0:1',   \n        'bootstrap_type': 'Bernoulli', \n    }\n    \n    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n    oof_tuned = np.zeros(len(y))\n    models_tuned = []\n    fold_metrics_tuned = []\n    \n    # Verbose TQDM for final training folds\n    for fold_idx, (train_idx, val_idx) in enumerate(tqdm(kf.split(X), total=N_FOLDS,\n                                                         desc=f\"  Final Training\", leave=False)):\n        final_params['random_seed'] = RANDOM_STATE + fold_idx\n        model = cb.CatBoostRegressor(**final_params)\n        \n        model.fit(\n            X[train_idx], y[train_idx],\n            eval_set=(X[val_idx], y[val_idx]),\n            verbose=False\n        )\n        \n        preds = model.predict(X[val_idx])\n        oof_tuned[val_idx] = preds\n        models_tuned.append(model)\n        \n        fold_rmse = np.sqrt(mean_squared_error(y[val_idx], preds))\n        fold_metrics_tuned.append(fold_rmse)\n        \n    final_rmse = np.sqrt(mean_squared_error(y, oof_tuned))\n    final_mae = mean_absolute_error(y, oof_tuned)\n    final_r2 = r2_score(y, oof_tuned)\n    \n    print(f\"\\n  FINAL TUNED RESULTS (5-Fold OOF):\")\n    print(f\"    RMSE: {final_rmse:.4f} (+/- {np.std(fold_metrics_tuned):.4f})\")\n    print(f\"    MAE:  {final_mae:.4f}\")\n    print(f\"    R:   {final_r2:.4f}\")\n    \n    # Compare with baseline\n    baseline_rmse = all_iteration_results[target][-1]['rmse']\n    improvement = baseline_rmse - final_rmse\n    print(f\"    Improvement over baseline: {improvement:+.4f}\")\n    \n    tuning_results[target] = {\n        'best_params': best_params,\n        'final_params': final_params,\n        'best_rmse': best_rmse, # Note: This is from the 80/20 split\n        'final_rmse': final_rmse, # This is from the full 5-fold OOF\n        'final_mae': final_mae,\n        'final_r2': final_r2,\n        'oof': oof_tuned,\n        'models': models_tuned,\n        'features': used_features,\n        'col_medians': col_medians,\n        'y': y,\n        'study': study,\n    }\n\n# Summary\nprint(\"\\n\" + \"=\" * 80)\nprint(\"HYPERPARAMETER TUNING SUMMARY\")\nprint(\"=\" * 80)\n\nprint(f\"\\n{'Target':<15} {'Baseline':>12} {'Tuned (OOF)':>12} {'Improvement':>12}\")\nprint(\"-\" * 55)\n\nfor target in CONFIG['targets']:\n    baseline = all_iteration_results[target][-1]['rmse']\n    tuned = tuning_results[target]['final_rmse']\n    improvement = baseline - tuned\n    print(f\"{target:<15} {baseline:>12.4f} {tuned:>12.4f} {improvement:>+12.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T17:34:27.606725Z","iopub.execute_input":"2025-11-30T17:34:27.606998Z","iopub.status.idle":"2025-11-30T20:05:04.554769Z","shell.execute_reply.started":"2025-11-30T17:34:27.606976Z","shell.execute_reply":"2025-11-30T20:05:04.553909Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nSTAGE 9: HYPERPARAMETER TUNING WITH OPTUNA (80/20 SPLIT)\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tuning Targets:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baef0f18281b40d785a143db6eac5e2e"}},"metadata":{}},{"name":"stdout","text":"\n============================================================\nTUNING O3_target\n============================================================\n  Features: 172, Samples: 171,679\n  Running 20 trials (80/20 Holdout)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  O3_target Trials:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8937256766dc48c9aaca5ddbe8d570cc"}},"metadata":{}},{"name":"stdout","text":"\n  Best RMSE (Val): 9.1869 (Trial 10)\n  Best Parameters:\n    iterations: 989\n    depth: 13\n    learning_rate: 0.093226\n    l2_leaf_reg: 9.500492\n    min_child_samples: 50\n    subsample: 0.915137\n\n  Training final model with best params (5-Fold CV)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  Final Training:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n  FINAL TUNED RESULTS (5-Fold OOF):\n    RMSE: 9.3247 (+/- 0.0740)\n    MAE:  5.6766\n    R:   0.9242\n    Improvement over baseline: +0.3040\n\n============================================================\nTUNING NO2_target\n============================================================\n  Features: 172, Samples: 171,679\n  Running 20 trials (80/20 Holdout)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  NO2_target Trials:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fa25ade362f4342a1e2f586c51c08f0"}},"metadata":{}},{"name":"stdout","text":"\n  Best RMSE (Val): 8.3929 (Trial 19)\n  Best Parameters:\n    iterations: 977\n    depth: 13\n    learning_rate: 0.074340\n    l2_leaf_reg: 1.083437\n    min_child_samples: 35\n    subsample: 0.714283\n\n  Training final model with best params (5-Fold CV)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  Final Training:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n  FINAL TUNED RESULTS (5-Fold OOF):\n    RMSE: 8.4042 (+/- 0.1340)\n    MAE:  5.4544\n    R:   0.9174\n    Improvement over baseline: +0.2717\n\n================================================================================\nHYPERPARAMETER TUNING SUMMARY\n================================================================================\n\nTarget              Baseline  Tuned (OOF)  Improvement\n-------------------------------------------------------\nO3_target             9.6287       9.3247      +0.3040\nNO2_target            8.6759       8.4042      +0.2717\n","output_type":"stream"}],"execution_count":27}]}