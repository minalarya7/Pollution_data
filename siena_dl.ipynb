{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vV9BiuT6zly"
   },
   "source": [
    "### Import all the essential libraries required to build, train, evaluate, and analyze a deep learning model, commonly used in EEG-based seizure detection tasks:\n",
    "\n",
    "\n",
    "-> NumPy is used for numerical computations and handling multi-dimensional arrays,\n",
    "which form the core structure of EEG signal data.\n",
    "\n",
    "->TensorFlow and Keras provide the deep learning framework, where layers and the Model class are used to design neural network architectures such as CNNs and BiLSTMs.\n",
    "\n",
    "->Training optimization is handled using callbacks like EarlyStopping, which stops training when validation performance no longer improves, ModelCheckpoint, which saves the best-performing model during training, and ReduceLROnPlateau, which lowers the learning rate when learning stagnates to improve convergence and prevent overfitting.\n",
    "\n",
    "->Scikit-learn is used for model validation and performance evaluation, with Leave-One-Out cross-validation ensuring robust testing by training on all samples except one iteratively, and evaluation metrics such as accuracy, precision, recall, F1-score, ROC-AUC, and confusion matrix measuring classification effectiveness.\n",
    "\n",
    "->Matplotlib and Seaborn are visualization libraries used to plot EEG signals, training curves, and confusion matrices for interpretability and analysis.\n",
    "\n",
    "->SciPy’s signal module provides signal processing tools such as filtering and spectral analysis, which are crucial for EEG preprocessing.\n",
    "\n",
    "->OS module supports file and directory management for dataset handling, while the warnings module suppresses non-critical runtime warnings to keep outputs clean and focused.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eNQSEaFJ6f1t"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 04:00:47.770600: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-15 04:00:47.843706: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-15 04:00:48.847290: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal as scipy_signal\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHGsrbsS7BHV"
   },
   "source": [
    "#GPU configuration:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGc2kpW0HYmd"
   },
   "source": [
    "###This function configures TensorFlow to efficiently use a GPU for deep learning training and prints detailed diagnostic information in a Jupyter Notebook text (Markdown) cell.\n",
    "\n",
    "\n",
    "->It first checks whether any physical GPUs are available using TensorFlow’s device listing utilities; if GPUs are detected, it enables **memory growth**, which allows TensorFlow to allocate GPU memory dynamically instead of reserving all memory at startup, preventing out-of-memory errors and improving resource sharing.\n",
    "\n",
    "->The function then reports the number and names of detected GPUs. To accelerate training on modern NVIDIA RTX GPUs, it enables **mixed precision training**, which uses both 16-bit (FP16) and 32-bit (FP32) floating-point computations; this leverages **Tensor Cores** to significantly speed up matrix operations while maintaining numerical stability.\n",
    "\n",
    "->**Soft device placement** is enabled so TensorFlow can automatically place operations on the GPU when possible and fall back to the CPU if needed.\n",
    "\n",
    "->The function also retrieves and prints low-level **GPU hardware details** such as compute capability, which helps verify compatibility and performance characteristics.\n",
    "\n",
    "->If GPU configuration is attempted after TensorFlow has already initialized devices, a runtime error is safely caught and reported. When no GPU is available, the function clearly indicates that computation will run on the CPU.\n",
    "\n",
    "->Finally, the function is called immediately so that GPU configuration is applied as soon as the notebook or script is executed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HZ-BihWn66dB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GPU CONFIGURATION\n",
      "================================================================================\n",
      "Found 1 GPU(s):\n",
      "  GPU 0: /physical_device:GPU:0\n",
      "Mixed precision enabled: mixed_float16\n",
      " (Utilizes Tensor Cores on RTX GPU for faster training)\n",
      "GPU acceleration enabled\n",
      "GPU Details: {'compute_capability': (8, 6), 'device_name': 'NVIDIA GeForce RTX 3060 Laptop GPU'}\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 04:00:52.455757: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-12-15 04:00:52.539921: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-12-15 04:00:52.540013: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-12-15 04:00:52.541716: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "def configure_gpu():\n",
    "    print(\"=\"*80)\n",
    "    print(\"GPU CONFIGURATION\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Check available GPUs\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Enable memory growth for all GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "            print(f\"Found {len(gpus)} GPU(s):\")\n",
    "            for i, gpu in enumerate(gpus):\n",
    "                print(f\"  GPU {i}: {gpu.name}\")\n",
    "\n",
    "            # Enable mixed precision training for RTX GPUs (faster training)\n",
    "            # RTX GPUs have Tensor Cores that accelerate FP16 operations\n",
    "            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "            tf.keras.mixed_precision.set_global_policy(policy)\n",
    "            print(f\"Mixed precision enabled: {policy.name}\")\n",
    "            print(\" (Utilizes Tensor Cores on RTX GPU for faster training)\")\n",
    "\n",
    "            # Set TensorFlow to use GPU\n",
    "            tf.config.set_soft_device_placement(True)\n",
    "            print(\"GPU acceleration enabled\")\n",
    "\n",
    "            # Display GPU compute capability\n",
    "            gpu_details = tf.config.experimental.get_device_details(gpus[0])\n",
    "            print(f\"GPU Details: {gpu_details}\")\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(f\"GPU configuration error: {e}\")\n",
    "    else:\n",
    "        print(\"No GPU found. Running on CPU.\")\n",
    "\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Call GPU configuration at import time\n",
    "configure_gpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ew9iXfeyCBlx"
   },
   "source": [
    "# STEP 1: DATA LOADING AND PREPARATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrJ2yBhpCPIL"
   },
   "source": [
    "###This function loads preprocessed EEG signal data and associated patient information in a format suitable for deep learning models and explains the rationale behind using .npz files.\n",
    "\n",
    " ->It scans a specified directory and loads all .npz files, where each file typically represents one patient and stores multiple related arrays in a single compressed container.\n",
    "\n",
    " ->For each file, the function extracts the patient ID from the filename, then loads the EEG signals—structured as multi-dimensional arrays with dimensions representing samples, channels, and time steps—and the corresponding binary labels indicating seizure or non-seizure events.\n",
    "\n",
    " ->These arrays are then appended to lists so that data from multiple patients can be processed together during training or cross-validation.\n",
    "\n",
    " ->The function also loads demographic embeddings from a separate .npy file, which encode patient-level information (such as age or gender) in a numerical format that can be fused with EEG features in neural networks.\n",
    "\n",
    " ->.npz files are used in deep learning because they efficiently store multiple NumPy arrays in a single compressed file, preserve exact numerical precision without loss, load significantly faster than text-based formats like CSV, and maintain consistent array shapes required by neural networks.\n",
    "\n",
    " ->This makes them ideal for large, high-dimensional data such as EEG signals, where fast I/O, memory efficiency, and structural integrity are critical for stable and reproducible model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mv8LYR5OCAlr"
   },
   "outputs": [],
   "source": [
    "def load_preprocessed_data(data_dir, demographic_file):\n",
    "\n",
    "    print(\"Loading preprocessed EEG data and demographic embeddings:\")\n",
    "\n",
    "    eeg_data = []\n",
    "    labels = []\n",
    "    patient_ids = []\n",
    "\n",
    "    # Load all .npz files from directory\n",
    "    for file in sorted(os.listdir(data_dir)):\n",
    "        if file.endswith('.npz'):\n",
    "            print(f\"Loading {file}....\")\n",
    "            patient_id = file.split('.')[0]\n",
    "            data = np.load(os.path.join(data_dir, file))\n",
    "\n",
    "            # eeg_data.append(data['eeg_signal'])  # Shape: (n_samples, n_channels, time_steps)\n",
    "            # labels.append(data['labels'])  # Shape: (n_samples,) - 0: non-seizure, 1: seizure\n",
    "            eeg_data.append(data['X'])  # Shape: (n_samples, n_channels, time_steps)\n",
    "            labels.append(data['y'])  # Shape: (n_samples,) - 0: non-seizure, 1: seizure\n",
    "            patient_ids.append(patient_id)\n",
    "            print(f\"Loaded {file}\")\n",
    "\n",
    "    # Load demographic embeddings\n",
    "    demographics = np.load(demographic_file)  # Shape: (14, 14)\n",
    "\n",
    "    print(f\"Loaded data for {len(patient_ids)} patients\")\n",
    "    print(f\"Demographics shape: {demographics.shape}\")\n",
    "\n",
    "    return eeg_data, labels, demographics, patient_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFf-YIckCvcJ"
   },
   "source": [
    "# STEP 2: TIME-FREQUENCY REPRESENTATION (STFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "as8ncstGCw6s"
   },
   "source": [
    "###Convert raw EEG time-domain signals into a **time–frequency representation** using the **Short-Time Fourier Transform (STFT)**, which is especially useful for seizure detection because seizures exhibit distinctive frequency patterns that change over time.\n",
    "\n",
    "->The `compute_stft` function takes a single EEG segment (organized by channels) and applies STFT independently to each channel using a specified sampling frequency (`fs`), window length (`nperseg`), and overlap (`noverlap`);\n",
    "\n",
    " ->STFT works by splitting the signal into short, overlapping time windows and applying the Fourier Transform to each window, allowing the model to observe how signal frequencies evolve over time rather than assuming stationarity.\n",
    "\n",
    " ->The complex STFT output is converted to magnitude values using the absolute function, since magnitude spectra capture signal energy distribution and are more meaningful for neural networks.\n",
    "\n",
    " ->The result is a 3D tensor representing channels, frequency bins, and time frames, which closely resembles image-like data suitable for CNNs.\n",
    "\n",
    " ->The `prepare_data_with_stft` function applies this transformation to the entire dataset patient-wise and segment-wise, enabling optional preprocessing control through the `use_stft` flag.\n",
    "\n",
    "  ->Using STFT enhances deep learning performance in EEG analysis because seizures are characterized by transient oscillations and spectral shifts that are difficult to detect in raw time-domain signals but become clearly separable in the time–frequency domain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QFpJ9gGHCmgc"
   },
   "outputs": [],
   "source": [
    "def compute_stft(eeg_segment, fs=256, nperseg=128, noverlap=64):\n",
    "    n_channels = eeg_segment.shape[0]\n",
    "    stft_results = []\n",
    "\n",
    "    for ch in range(n_channels):\n",
    "        f, t, Zxx = scipy_signal.stft(eeg_segment[ch], fs=fs,\n",
    "                                       nperseg=nperseg, noverlap=noverlap)\n",
    "        stft_results.append(np.abs(Zxx))\n",
    "\n",
    "    return np.array(stft_results)  # Shape: (n_channels, freq_bins, time_frames)\n",
    "\n",
    "\n",
    "def prepare_data_with_stft(eeg_data, use_stft=True):\n",
    "    if not use_stft:\n",
    "        return eeg_data\n",
    "\n",
    "    print(\"Computing STFT for time-frequency representation...\")\n",
    "    processed_data = []\n",
    "\n",
    "    for patient_data in eeg_data:\n",
    "        patient_stft = []\n",
    "        for segment in patient_data:\n",
    "            stft_segment = compute_stft(segment)\n",
    "            patient_stft.append(stft_segment)\n",
    "        processed_data.append(np.array(patient_stft))\n",
    "\n",
    "    return processed_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l28RUZZ3DDBN"
   },
   "source": [
    "# STEP 3: DATA AUGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-NcgnlTDGGJ"
   },
   "source": [
    "###This function performs **data augmentation** on EEG segments to artificially increase dataset diversity and improve the generalization ability of deep learning models.\n",
    "\n",
    "->It first creates a copy of the original EEG segment to preserve the raw data, then applies one of several biologically plausible transformations based on the selected augmentation type.\n",
    "\n",
    "->In the **noise augmentation**, small Gaussian noise is added to simulate real-world recording disturbances such as sensor noise or environmental interference, helping the model become robust to slight signal variations.\n",
    "\n",
    "->The **time-shift augmentation** randomly shifts the EEG signal along the time axis, which teaches the model that seizure patterns are invariant to small temporal misalignments and reduces sensitivity to exact onset positions.\n",
    "\n",
    "->The **channel dropout augmentation** randomly zeros out a subset of EEG channels, mimicking electrode failures or poor contact and encouraging the model to learn spatially distributed patterns rather than relying on a few dominant channels.\n",
    "\n",
    "->Overall, these augmentation strategies help prevent overfitting, improve robustness to noise and missing data, and make the model more reliable when applied to unseen EEG recordings in real clinical settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Was_xLcnDQwm"
   },
   "outputs": [],
   "source": [
    "def augment_eeg_data(eeg_segment, augmentation_type='noise'):\n",
    "    augmented = eeg_segment.copy()\n",
    "\n",
    "    if augmentation_type == 'noise':\n",
    "        # Add Gaussian noise (SNR ~20dB)\n",
    "        noise = np.random.normal(0, 0.1, augmented.shape)\n",
    "        augmented = augmented + noise\n",
    "\n",
    "    elif augmentation_type == 'shift':\n",
    "        # Time shifting\n",
    "        shift = np.random.randint(-10, 10)\n",
    "        augmented = np.roll(augmented, shift, axis=-1)\n",
    "\n",
    "    elif augmentation_type == 'dropout':\n",
    "        # Random channel dropout (10% of channels)\n",
    "        n_channels = augmented.shape[0]\n",
    "        dropout_channels = np.random.choice(n_channels,\n",
    "                                           size=int(0.1 * n_channels),\n",
    "                                           replace=False)\n",
    "        augmented[dropout_channels] = 0\n",
    "\n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1cJWZQDDgqP"
   },
   "source": [
    "# STEP 4: ATTENTION MECHANISM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoKwTiFDDdfr"
   },
   "source": [
    "###This custom **AttentionLayer** implements a learnable attention mechanism that allows a neural network to focus on the most informative time steps in sequential EEG data, which is crucial for seizure detection where only certain temporal regions contain discriminative seizure activity.\n",
    "\n",
    "->The layer extends Keras’s base `Layer` class and defines trainable parameters in the `build` method, including a **weight matrix** initialized using Glorot (Xavier) initialization for stable gradient flow and a **bias vector** initialized to zeros.\n",
    "\n",
    "->During the forward pass in the `call` method, the input tensor—structured as batches of time sequences with extracted features—is linearly transformed and passed through a **tanh activation** to produce attention scores that capture the relevance of each time step.\n",
    "\n",
    "->These scores are normalized using a **softmax function** across the temporal dimension, converting them into attention weights that sum to one and represent the relative importance of each time step.\n",
    "\n",
    "->The original inputs are then scaled by these weights through element-wise multiplication, emphasizing seizure-relevant temporal patterns while suppressing less informative background activity.\n",
    "\n",
    "->The `get_config` method ensures that the layer can be properly serialized and reloaded, making it compatible with model saving and deployment workflows in deep learning applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LOupcd3sDX-F"
   },
   "outputs": [],
   "source": [
    "class AttentionLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Attention weight matrix\n",
    "        self.W = self.add_weight(name='attention_weight',\n",
    "                                shape=(input_shape[-1], input_shape[-1]),\n",
    "                                initializer='glorot_uniform',\n",
    "                                trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias',\n",
    "                                shape=(input_shape[-1],),\n",
    "                                initializer='zeros',\n",
    "                                trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Compute attention scores\n",
    "        # inputs shape: (batch_size, time_steps, features)\n",
    "        attention_scores = tf.nn.tanh(tf.matmul(inputs, self.W) + self.b)\n",
    "        attention_weights = tf.nn.softmax(attention_scores, axis=1)\n",
    "\n",
    "        # Apply attention weights\n",
    "        attended_output = inputs * attention_weights\n",
    "\n",
    "        return attended_output\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(AttentionLayer, self).get_config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJvzs8q-EAKz"
   },
   "source": [
    "# STEP 5: CNN-BiLSTM MODEL WITH ATTENTION AND DEMOGRAPHICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m10HO1_KD8dB"
   },
   "source": [
    "###This function builds a **hybrid CNN–BiLSTM deep learning model with attention and demographic fusion** for EEG-based seizure detection, combining spatial, temporal, and patient-specific information in a single architecture.\n",
    "\n",
    "->The model uses two inputs: one for EEG signals and one for demographic embeddings, allowing it to learn both signal-level and subject-level patterns. If STFT features are used, the EEG input is reshaped to make the time–frequency representation compatible with one-dimensional convolutions.\n",
    "\n",
    "->The **CNN blocks** consist of stacked Conv1D layers with increasing filter sizes that learn local spatial patterns across EEG channels and time, such as rhythmic discharges or spike-like activity, while **batch normalization** stabilizes training, **max pooling** reduces temporal resolution and noise, and **dropout** prevents overfitting.\n",
    "\n",
    "->The extracted features are then passed to **Bidirectional LSTM layers**, which model long-range temporal dependencies in both forward and backward directions, enabling the network to capture seizure onset, evolution, and offset patterns more effectively.\n",
    "\n",
    "->An **attention mechanism** is applied next to dynamically emphasize the most seizure-relevant time segments, improving interpretability and detection accuracy. Global average pooling compresses the attended temporal features into a fixed-length representation.\n",
    "\n",
    "->In parallel, demographic data is processed through a dense layer to learn compact patient embeddings, which are concatenated with EEG-derived features to incorporate patient-specific variability.\n",
    "\n",
    " ->The **classification head** consists of fully connected layers with dropout for robust feature learning, and the final softmax output layer performs binary classification between seizure and non-seizure classes.\n",
    "\n",
    " ->The model is compiled using the **Adam optimizer** for efficient gradient-based learning, **sparse categorical cross-entropy loss** suitable for integer class labels, and evaluation metrics including accuracy, precision, and recall to assess clinical relevance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "d2cAdbLZDolN"
   },
   "outputs": [],
   "source": [
    "def build_cnn_bilstm_model(input_shape, demographic_dim=14, use_stft=False):\n",
    "\n",
    "    # Input Layers\n",
    "    eeg_input = layers.Input(shape=input_shape, name='eeg_input')\n",
    "    demographic_input = layers.Input(shape=(demographic_dim,), name='demographic_input')\n",
    "\n",
    "    # CNN Layers for Spatial Feature Extraction\n",
    "    # Extract spatial patterns across EEG channels\n",
    "    if use_stft:\n",
    "        # For STFT input: (channels, freq_bins, time_frames)\n",
    "        x = layers.Reshape((input_shape[0], -1))(eeg_input)\n",
    "    else:\n",
    "        x = eeg_input\n",
    "\n",
    "    # First Conv1D block\n",
    "    x = layers.Conv1D(filters=64, kernel_size=5, padding='same',\n",
    "                     activation='relu', name='conv1')(x)\n",
    "    x = layers.BatchNormalization(name='bn1')(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2, name='pool1')(x)\n",
    "    x = layers.Dropout(0.3, name='dropout1')(x)\n",
    "\n",
    "    # Second Conv1D block\n",
    "    x = layers.Conv1D(filters=128, kernel_size=5, padding='same',\n",
    "                     activation='relu', name='conv2')(x)\n",
    "    x = layers.BatchNormalization(name='bn2')(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2, name='pool2')(x)\n",
    "    x = layers.Dropout(0.3, name='dropout2')(x)\n",
    "\n",
    "    # Third Conv1D block\n",
    "    x = layers.Conv1D(filters=256, kernel_size=3, padding='same',\n",
    "                     activation='relu', name='conv3')(x)\n",
    "    x = layers.BatchNormalization(name='bn3')(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2, name='pool3')(x)\n",
    "    x = layers.Dropout(0.3, name='dropout3')(x)\n",
    "\n",
    "    #  BiLSTM Layers for Temporal Feature Extraction\n",
    "    # Capture past and future temporal dependencies\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True,\n",
    "                                         name='bilstm1'), name='bi_lstm1')(x)\n",
    "    x = layers.Dropout(0.4, name='dropout4')(x)\n",
    "\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True,\n",
    "                                         name='bilstm2'), name='bi_lstm2')(x)\n",
    "    x = layers.Dropout(0.4, name='dropout5')(x)\n",
    "\n",
    "    # Attention Mechanism\n",
    "    # Focus on seizure-relevant temporal and spatial patterns\n",
    "    x = AttentionLayer(name='attention')(x)\n",
    "\n",
    "    # Global pooling to aggregate temporal information\n",
    "    x = layers.GlobalAveragePooling1D(name='global_pool')(x)\n",
    "\n",
    "    # Demographic Integration\n",
    "    # Concatenate demographic embeddings with extracted features\n",
    "    # This helps model learn patient-specific patterns\n",
    "    demographic_dense = layers.Dense(32, activation='relu',\n",
    "                                    name='demographic_dense')(demographic_input)\n",
    "\n",
    "    # Concatenate EEG features with demographic features\n",
    "    combined = layers.Concatenate(name='concatenate')([x, demographic_dense])\n",
    "\n",
    "    # Classification Head\n",
    "    combined = layers.Dense(128, activation='relu', name='dense1')(combined)\n",
    "    combined = layers.Dropout(0.5, name='dropout6')(combined)\n",
    "\n",
    "    combined = layers.Dense(64, activation='relu', name='dense2')(combined)\n",
    "    combined = layers.Dropout(0.5, name='dropout7')(combined)\n",
    "\n",
    "    # Output layer: Binary classification (seizure vs non-seizure)\n",
    "    output = layers.Dense(2, activation='softmax', name='output')(combined)\n",
    "\n",
    "    # Build and Compile Model\n",
    "    model = Model(inputs=[eeg_input, demographic_input], outputs=output)\n",
    "\n",
    "    # Compile with Adam optimizer and categorical crossentropy loss\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4wbbznoEWq3"
   },
   "source": [
    "# STEP 6: LEAVE-ONE-PATIENT-OUT (LOPO) CROSS-VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YeKOYwpMEbAh"
   },
   "source": [
    "###This function implements **Leave-One-Patient-Out (LOPO) cross-validation**, a rigorous evaluation strategy widely used in EEG-based seizure detection to ensure true patient-independent generalization.\n",
    "\n",
    "->In this approach, data from one patient is held out as the test set while the model is trained on data from all remaining patients, and this process is repeated so that each patient serves as the test subject exactly once.\n",
    "\n",
    " ->For every fold, the function concatenates EEG segments and labels from the training patients, aligns demographic embeddings with each EEG segment by repetition, and keeps the held-out patient’s data strictly unseen during training.\n",
    "\n",
    "-> A CNN–BiLSTM–Attention model is then built and trained using GPU acceleration, with multiple **model checkpoints** to save the best-performing models based on validation accuracy and validation loss, as well as periodic snapshots to safeguard against training interruptions.\n",
    "\n",
    " ->**Early stopping** halts training when validation loss stops improving, preventing overfitting, while **learning rate reduction on plateau** improves convergence by lowering the learning rate when optimization stagnates.\n",
    "\n",
    " ->**TensorBoard logging** enables real-time monitoring of GPU usage, losses, and metrics.\n",
    "\n",
    " ->After training, the best model (based on validation accuracy) is reloaded and evaluated on the held-out patient using clinically meaningful metrics such as accuracy, precision, recall, F1-score, AUC–ROC, and the confusion matrix.\n",
    "\n",
    " ->These metrics are stored fold-wise to provide a comprehensive performance summary across all patients.\n",
    "\n",
    " ->Overall, this LOPO framework closely mimics real-world clinical deployment, where models must generalize to entirely unseen patients rather than benefiting from subject-specific data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dXkcQ7z7Ehny"
   },
   "outputs": [],
   "source": [
    "def lopo_cross_validation(eeg_data, labels, demographics, patient_ids,\n",
    "                          input_shape, epochs=50, batch_size=32):\n",
    "    n_patients = len(patient_ids)\n",
    "    results = {\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1_score': [],\n",
    "        'auc_roc': [],\n",
    "        'confusion_matrices': []\n",
    "    }\n",
    "\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    checkpoint_dir = './model_checkpoints'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(f\"Model checkpoints will be saved to: {checkpoint_dir}\")\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Starting LOPO Cross-Validation with {n_patients} patients\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # Iterate through each patient as test set\n",
    "    for test_idx in range(n_patients):\n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"FOLD {test_idx + 1}/{n_patients}: Testing on Patient {patient_ids[test_idx]}\")\n",
    "        print(f\"{'─'*80}\")\n",
    "\n",
    "        # Split data: Train on all except one patient\n",
    "        train_indices = [i for i in range(n_patients) if i != test_idx]\n",
    "\n",
    "        X_train = np.concatenate([eeg_data[i] for i in train_indices], axis=0)\n",
    "        y_train = np.concatenate([labels[i] for i in train_indices], axis=0)\n",
    "        demo_train = np.concatenate([np.tile(demographics[i], (len(labels[i]), 1))\n",
    "                                     for i in train_indices], axis=0)\n",
    "\n",
    "        X_test = eeg_data[test_idx]\n",
    "        y_test = labels[test_idx]\n",
    "        demo_test = np.tile(demographics[test_idx], (len(y_test), 1))\n",
    "\n",
    "        print(f\"Training samples: {len(X_train)} | Testing samples: {len(X_test)}\")\n",
    "        print(f\"Training seizures: {np.sum(y_train)} | Testing seizures: {np.sum(y_test)}\")\n",
    "\n",
    "        # Build Model\n",
    "        model = build_cnn_bilstm_model(input_shape=input_shape,\n",
    "                                       demographic_dim=demographics.shape[1])\n",
    "\n",
    "        #Create multiple checkpoints\n",
    "\n",
    "        # Checkpoint 1: Save best model based on validation ACCURACY\n",
    "        checkpoint_acc = ModelCheckpoint(\n",
    "            filepath=os.path.join(checkpoint_dir, f'best_model_fold_{test_idx+1}_val_acc.h5'),\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',  # Maximize accuracy\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,  # Save entire model architecture + weights\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Checkpoint 2: Save best model based on validation LOSS\n",
    "        checkpoint_loss = ModelCheckpoint(\n",
    "            filepath=os.path.join(checkpoint_dir, f'best_model_fold_{test_idx+1}_val_loss.h5'),\n",
    "            monitor='val_loss',\n",
    "            mode='min',  # Minimize loss\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Checkpoint 3: Save model at every epoch\n",
    "        checkpoint_epoch = ModelCheckpoint(\n",
    "            filepath=os.path.join(checkpoint_dir, f'model_fold_{test_idx+1}_epoch_{{epoch:02d}}_acc_{{val_accuracy:.4f}}.h5'),\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            save_best_only=False,  # Save every epoch\n",
    "            save_weights_only=False,\n",
    "            period=5,  # Save every 5 epochs to avoid too many files\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Early stopping: Stop training if validation loss doesn't improve and model performance plateaus\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Reduce learning rate when validation loss plateaus\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # TensorBoard logging for GPU monitoring and training visualization\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=os.path.join(checkpoint_dir, f'logs/fold_{test_idx+1}'),\n",
    "            histogram_freq=1,\n",
    "            write_graph=True,\n",
    "            update_freq='epoch'\n",
    "        )\n",
    "\n",
    "        # Train Model with GPU Acceleration\n",
    "        print(f\"\\nTraining model for fold {test_idx + 1} on GPU...\")\n",
    "        print(\"Monitoring: val_accuracy (↑) and val_loss (↓)\")\n",
    "        print(f\"Checkpoints saving to: {checkpoint_dir}/\")\n",
    "\n",
    "        history = model.fit(\n",
    "            [X_train, demo_train], y_train,\n",
    "            validation_split=0.2,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[\n",
    "                checkpoint_acc,      # Save best accuracy model\n",
    "                checkpoint_loss,     # Save best loss model\n",
    "                checkpoint_epoch,    # Save periodic checkpoints\n",
    "                early_stop,          # Early stopping\n",
    "                reduce_lr,           # Learning rate reduction\n",
    "                tensorboard_callback # TensorBoard logging\n",
    "            ],\n",
    "            verbose=1,\n",
    "            use_multiprocessing=True,  # Enable multiprocessing for data loading\n",
    "            workers=4  # Number of parallel workers\n",
    "        )\n",
    "\n",
    "        # Load Best Model for Evaluation\n",
    "        print(f\"\\nLoading best model based on validation accuracy...\")\n",
    "        best_model_path = os.path.join(checkpoint_dir, f'best_model_fold_{test_idx+1}_val_acc.h5')\n",
    "        model = keras.models.load_model(best_model_path, custom_objects={'AttentionLayer': AttentionLayer})\n",
    "\n",
    "        #Evaluate on Test Patient\n",
    "        print(f\"\\nEvaluating on test patient {patient_ids[test_idx]}...\")\n",
    "        y_pred_probs = model.predict([X_test, demo_test], verbose=0)\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "        # Calculate Metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "        # AUC-ROC (only if both classes present in test set)\n",
    "        try:\n",
    "            auc = roc_auc_score(y_test, y_pred_probs[:, 1])\n",
    "        except:\n",
    "            auc = 0.0\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        # Store results\n",
    "        results['accuracy'].append(accuracy)\n",
    "        results['precision'].append(precision)\n",
    "        results['recall'].append(recall)\n",
    "        results['f1_score'].append(f1)\n",
    "        results['auc_roc'].append(auc)\n",
    "        results['confusion_matrices'].append(cm)\n",
    "\n",
    "        # Print fold results\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"FOLD {test_idx + 1} RESULTS:\")\n",
    "        print(f\"{'='*40}\")\n",
    "        print(f\"Accuracy:  {accuracy*100:.2f}%\")\n",
    "        print(f\"Precision: {precision*100:.2f}%\")\n",
    "        print(f\"Recall:    {recall*100:.2f}%\")\n",
    "        print(f\"F1-Score:  {f1*100:.2f}%\")\n",
    "        print(f\"AUC-ROC:   {auc:.4f}\")\n",
    "        print(f\"\\nConfusion Matrix:\")\n",
    "        print(cm)\n",
    "        print(f\"\\nBest model saved at: {best_model_path}\")\n",
    "        print(f\"{'='*40}\\n\")\n",
    "\n",
    "        # Clean up\n",
    "        del model\n",
    "        keras.backend.clear_session()\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"All model checkpoints saved in: {checkpoint_dir}/\")\n",
    "    print(f\"TensorBoard logs saved in: {checkpoint_dir}/logs/\")\n",
    "    print(f\"To view training logs, run: tensorboard --logdir={checkpoint_dir}/logs/\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9bA8f4vFAn3"
   },
   "source": [
    "# STEP 7: RESULTS ANALYSIS AND VISUALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TV7ZBdtFDDv"
   },
   "source": [
    "###This function summarizes and interprets the results obtained from **Leave-One-Patient-Out (LOPO) cross-validation**, providing both quantitative performance statistics and visual insights into model behavior across patients.\n",
    "\n",
    " ->It first computes the **mean and standard deviation** of key evaluation metrics—accuracy, precision, recall, F1-score, and AUC–ROC—across all folds, which reflects how consistently the model performs when tested on unseen patients.\n",
    "\n",
    " ->The function then checks whether a predefined **target accuracy of 95%** has been achieved, offering practical guidance if the target is not met by suggesting strategies such as additional data augmentation or hyperparameter tuning.\n",
    "\n",
    " ->To enhance interpretability, it generates multiple visualizations:\n",
    "  a line plot showing how performance metrics vary across patients,\n",
    "  a box plot illustrating the distribution and variability of metrics,\n",
    "  an **average confusion matrix** that highlights overall classification behavior between seizure and non-seizure classes,\n",
    "  a bar chart displaying **per-patient accuracy** to identify subjects for whom the model performs better or worse.\n",
    "  \n",
    "  ->These plots help detect inter-patient variability, potential bias, and robustness of the model.\n",
    "  \n",
    "  ->Finally, the function saves all visual outputs as a high-resolution image file, making it suitable for reporting, research documentation, and clinical analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "u4Dc4MoUFHUd"
   },
   "outputs": [],
   "source": [
    "def analyze_lopo_results(results, patient_ids):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"OVERALL LOPO CROSS-VALIDATION RESULTS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # Calculate mean and std for each metric\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']\n",
    "\n",
    "    for metric in metrics:\n",
    "        values = np.array(results[metric])\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        print(f\"{metric.upper():12s}: {mean_val*100:.2f}% ± {std_val*100:.2f}%\")\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "\n",
    "    # Check if target accuracy achieved\n",
    "    mean_accuracy = np.mean(results['accuracy'])\n",
    "    if mean_accuracy >= 0.95:\n",
    "        print(f\"\\n✓ TARGET ACHIEVED: Model accuracy ({mean_accuracy*100:.2f}%) >= 95%\")\n",
    "    else:\n",
    "        print(f\"\\n✗ Target not met: Model accuracy ({mean_accuracy*100:.2f}%) < 95%\")\n",
    "        print(\"   Consider: More data augmentation, hyperparameter tuning, or longer training\")\n",
    "\n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # Plot 1: Metrics per fold\n",
    "    ax1 = axes[0, 0]\n",
    "    fold_indices = np.arange(1, len(patient_ids) + 1)\n",
    "    ax1.plot(fold_indices, np.array(results['accuracy'])*100, 'o-', label='Accuracy', linewidth=2)\n",
    "    ax1.plot(fold_indices, np.array(results['precision'])*100, 's-', label='Precision', linewidth=2)\n",
    "    ax1.plot(fold_indices, np.array(results['recall'])*100, '^-', label='Recall', linewidth=2)\n",
    "    ax1.plot(fold_indices, np.array(results['f1_score'])*100, 'd-', label='F1-Score', linewidth=2)\n",
    "    ax1.axhline(y=95, color='r', linestyle='--', label='95% Target')\n",
    "    ax1.set_xlabel('Fold (Patient)', fontsize=12)\n",
    "    ax1.set_ylabel('Score (%)', fontsize=12)\n",
    "    ax1.set_title('Performance Metrics per LOPO Fold', fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Box plot of metrics\n",
    "    ax2 = axes[0, 1]\n",
    "    box_data = [np.array(results[m])*100 for m in ['accuracy', 'precision', 'recall', 'f1_score']]\n",
    "    bp = ax2.boxplot(box_data, labels=['Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "    ax2.axhline(y=95, color='r', linestyle='--', label='95% Target')\n",
    "    ax2.set_ylabel('Score (%)', fontsize=12)\n",
    "    ax2.set_title('Distribution of Performance Metrics', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Plot 3: Average confusion matrix\n",
    "    ax3 = axes[1, 0]\n",
    "    avg_cm = np.mean(results['confusion_matrices'], axis=0)\n",
    "    sns.heatmap(avg_cm, annot=True, fmt='.1f', cmap='Blues', ax=ax3,\n",
    "                xticklabels=['Non-Seizure', 'Seizure'],\n",
    "                yticklabels=['Non-Seizure', 'Seizure'])\n",
    "    ax3.set_title('Average Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    ax3.set_ylabel('True Label', fontsize=12)\n",
    "    ax3.set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "    # Plot 4: Per-patient performance\n",
    "    ax4 = axes[1, 1]\n",
    "    patient_labels = [f'P{i+1}' for i in range(len(patient_ids))]\n",
    "    x_pos = np.arange(len(patient_labels))\n",
    "    ax4.bar(x_pos, np.array(results['accuracy'])*100, alpha=0.7, color='steelblue')\n",
    "    ax4.axhline(y=95, color='r', linestyle='--', linewidth=2, label='95% Target')\n",
    "    ax4.set_xlabel('Patient', fontsize=12)\n",
    "    ax4.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax4.set_title('Per-Patient Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xticks(x_pos)\n",
    "    ax4.set_xticklabels(patient_labels, rotation=45)\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lopo_results.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nVisualization saved as 'lopo_results.png'\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMXMrpdgFR2u"
   },
   "source": [
    "# STEP 8: MAIN EXECUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ06U11FFULC"
   },
   "source": [
    "Main execution function for seizure detection with LOPO cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File and parameter configuration\n",
    "DATA_DIR = \"preprocessed_data_2\"      # Directory with .npz files\n",
    "DEMOGRAPHIC_FILE = \"demographic_embeddings.npy\"\n",
    "\n",
    "USE_STFT = False\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed EEG data and demographic embeddings:\n",
      "Loading PN00-1.npz....\n",
      "Loaded PN00-1.npz\n",
      "Loading PN00-2.npz....\n",
      "Loaded PN00-2.npz\n",
      "Loading PN00-3.npz....\n",
      "Loaded PN00-3.npz\n",
      "Loading PN00-4.npz....\n",
      "Loaded PN00-4.npz\n",
      "Loading PN00-5.npz....\n",
      "Loaded PN00-5.npz\n",
      "Loading PN01-1.npz....\n",
      "Loaded PN01-1.npz\n",
      "Loading PN03-1.npz....\n",
      "Loaded PN03-1.npz\n",
      "Loading PN03-2.npz....\n",
      "Loaded PN03-2.npz\n",
      "Loading PN05-2.npz....\n",
      "Loaded PN05-2.npz\n",
      "Loading PN05-3.npz....\n",
      "Loaded PN05-3.npz\n",
      "Loading PN05-4.npz....\n",
      "Loaded PN05-4.npz\n",
      "Loading PN06-1.npz....\n",
      "Loaded PN06-1.npz\n",
      "Loading PN06-2.npz....\n",
      "Loaded PN06-2.npz\n",
      "Loading PN06-3.npz....\n",
      "Loaded PN06-3.npz\n",
      "Loading PN06-4.npz....\n",
      "Loaded PN06-4.npz\n",
      "Loading PN06-5.npz....\n",
      "Loaded PN06-5.npz\n",
      "Loading PN07-1.npz....\n",
      "Loaded PN07-1.npz\n",
      "Loading PN09-1.npz....\n",
      "Loaded PN09-1.npz\n",
      "Loading PN09-2.npz....\n",
      "Loaded PN09-2.npz\n",
      "Loading PN09-3.npz....\n",
      "Loaded PN09-3.npz\n",
      "Loading PN10-1.npz....\n",
      "Loaded PN10-1.npz\n",
      "Loading PN10-10.npz....\n",
      "Loaded PN10-10.npz\n",
      "Loading PN10-2.npz....\n",
      "Loaded PN10-2.npz\n",
      "Loading PN10-3.npz....\n",
      "Loaded PN10-3.npz\n",
      "Loading PN10-4.5.6.npz....\n",
      "Loaded PN10-4.5.6.npz\n",
      "Loading PN10-7.8.9.npz....\n",
      "Loaded PN10-7.8.9.npz\n",
      "Loading PN11-1.npz....\n",
      "Loaded PN11-1.npz\n",
      "Loading PN12-1.2.npz....\n",
      "Loaded PN12-1.2.npz\n",
      "Loading PN12-3.npz....\n",
      "Loaded PN12-3.npz\n",
      "Loading PN12-4.npz....\n",
      "Loaded PN12-4.npz\n",
      "Loading PN13-1.npz....\n",
      "Loaded PN13-1.npz\n",
      "Loading PN13-2.npz....\n",
      "Loaded PN13-2.npz\n",
      "Loading PN13-3.npz....\n",
      "Loaded PN13-3.npz\n",
      "Loading PN14-1.npz....\n",
      "Loaded PN14-1.npz\n",
      "Loading PN14-2.npz....\n",
      "Loaded PN14-2.npz\n",
      "Loading PN14-3.npz....\n",
      "Loaded PN14-3.npz\n",
      "Loading PN14-4.npz....\n",
      "Loaded PN14-4.npz\n",
      "Loading PN16-1.npz....\n",
      "Loaded PN16-1.npz\n",
      "Loading PN16-2.npz....\n",
      "Loaded PN16-2.npz\n",
      "Loading PN17-1.npz....\n",
      "Loaded PN17-1.npz\n",
      "Loading PN17-2.npz....\n",
      "Loaded PN17-2.npz\n",
      "Loaded data for 41 patients\n",
      "Demographics shape: (14, 14)\n"
     ]
    }
   ],
   "source": [
    "eeg_data, labels, demographics, patient_ids = load_preprocessed_data(\n",
    "    DATA_DIR,\n",
    "    DEMOGRAPHIC_FILE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_STFT:\n",
    "    eeg_data = prepare_data_with_stft(eeg_data, use_stft=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG data: 41\n",
      "Labels: 41\n",
      "Demographics: 14\n",
      "Patient IDs: 41\n"
     ]
    }
   ],
   "source": [
    "print(\"EEG data:\", len(eeg_data))\n",
    "print(\"Labels:\", len(labels))\n",
    "print(\"Demographics:\", len(demographics))\n",
    "print(\"Patient IDs:\", len(patient_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'demographic_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# demographics: (14, D)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# demographic_ids: list of patient IDs used when embeddings were created\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# YOU MUST have this — if not, see note below\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(\u001b[43mdemographic_ids\u001b[49m), \u001b[38;5;28mlen\u001b[39m(demographic_ids))\n",
      "\u001b[31mNameError\u001b[39m: name 'demographic_ids' is not defined"
     ]
    }
   ],
   "source": [
    "# demographics: (14, D)\n",
    "# demographic_ids: list of patient IDs used when embeddings were created\n",
    "\n",
    "# YOU MUST have this — if not, see note below\n",
    "print(type(demographic_ids), len(demographic_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape per sample: (27, 1280)\n"
     ]
    }
   ],
   "source": [
    "# Assumes all patients have same shape: (n_samples, n_channels, time_steps)\n",
    "input_shape = eeg_data[0].shape[1:]\n",
    "print(\"Input shape per sample:\", input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m aligned_demographics = []\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pid \u001b[38;5;129;01min\u001b[39;00m patient_ids:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mpid\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdemographics\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m      9\u001b[39m         aligned_demographics.append(demographics[pid])\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: '<' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "# ---- FIX: align demographics with patients ----\n",
    "\n",
    "D = demographics.shape[1]  # embedding dimension\n",
    "\n",
    "aligned_demographics = []\n",
    "\n",
    "for pid in patient_ids:\n",
    "    if pid < len(demographics):\n",
    "        aligned_demographics.append(demographics[pid])\n",
    "    else:\n",
    "        aligned_demographics.append(np.zeros(D, dtype=np.float32))\n",
    "\n",
    "demographics = np.stack(aligned_demographics)\n",
    "\n",
    "print(\"After alignment:\")\n",
    "print(len(eeg_data), len(labels), len(demographics), len(patient_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoints will be saved to: ./model_checkpoints\n",
      "\n",
      "================================================================================\n",
      "Starting LOPO Cross-Validation with 41 patients\n",
      "================================================================================\n",
      "\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "FOLD 1/41: Testing on Patient PN00-1\n",
      "────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 14 is out of bounds for axis 0 with size 14",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results = \u001b[43mlopo_cross_validation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43meeg_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43meeg_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdemographics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdemographics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatient_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatient_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mlopo_cross_validation\u001b[39m\u001b[34m(eeg_data, labels, demographics, patient_ids, input_shape, epochs, batch_size)\u001b[39m\n\u001b[32m     31\u001b[39m X_train = np.concatenate([eeg_data[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_indices], axis=\u001b[32m0\u001b[39m)\n\u001b[32m     32\u001b[39m y_train = np.concatenate([labels[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_indices], axis=\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m demo_train = np.concatenate(\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdemographics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m]\u001b[49m, axis=\u001b[32m0\u001b[39m)\n\u001b[32m     36\u001b[39m X_test = eeg_data[test_idx]\n\u001b[32m     37\u001b[39m y_test = labels[test_idx]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     31\u001b[39m X_train = np.concatenate([eeg_data[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_indices], axis=\u001b[32m0\u001b[39m)\n\u001b[32m     32\u001b[39m y_train = np.concatenate([labels[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_indices], axis=\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m demo_train = np.concatenate([np.tile(\u001b[43mdemographics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m, (\u001b[38;5;28mlen\u001b[39m(labels[i]), \u001b[32m1\u001b[39m))\n\u001b[32m     34\u001b[39m                              \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_indices], axis=\u001b[32m0\u001b[39m)\n\u001b[32m     36\u001b[39m X_test = eeg_data[test_idx]\n\u001b[32m     37\u001b[39m y_test = labels[test_idx]\n",
      "\u001b[31mIndexError\u001b[39m: index 14 is out of bounds for axis 0 with size 14"
     ]
    }
   ],
   "source": [
    "results = lopo_cross_validation(\n",
    "    eeg_data=eeg_data,\n",
    "    labels=labels,\n",
    "    demographics=demographics,\n",
    "    patient_ids=patient_ids,\n",
    "    input_shape=input_shape,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_lopo_results(results, patient_ids)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LOPO Cross-Validation Complete!\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
